\chapter{Training LLMs: How to generate the generator}

\section{Multi-GPU environments}
Training is a resource-intensive endeavor. A model that only takes a single GPU to run inference on may take 10 times that many to train if, for nothing else, to parallelize your work and speed things up so you aren't waiting for a thousand years for it to finish training.  

\subsection{Setting up} 
It should be pointed out up front that while multi-GPU environments are powerful, they are also expensive. 
