
\section{Flash Attention}
\label{sec:transformer:flash_attention}

\chapter{Positional Embeddings}
\label{ch:transformer:posemb}

Rather than focusing on a token's absolute position in a sentence, relative positional embeddings concentrate on the distances between pairs of tokens. This method doesn't add a position vector to the word vector directly. Instead, it alters the attention mechanism to incorporate relative positional information.

\subsection{Rotary Positional Embeddings}

\chapter{Tokenization}
\label{ch:transformer:tokenization}

\chapter{Model Compression}
\label{ch:transformer:compression}


