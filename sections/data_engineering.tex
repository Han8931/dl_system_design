\chapter{Data Engineering for LLMs}

\textit{Data engineering} is the development, implementation, and maintenance of systems and processes that take in raw data and produce high-quality, consistent information that supports downstream use cases, such as analysis and machine learning. 

There isn't more valuable asset than your data. All successful AI and ML initiatives are built on a good data engineering foundation. It's important then that we acquire, clean, and curate our data. 
% Unlike other ML models, you generally won't be starting from scratch when creating an LLM customized for your specific task. 

\section{Models and the Foundation}
The most important dataset you will need to collect when training is the model weights of a pretrained model. 

\subsection{Evaluating LLMs}
When evaluating a model, you will need two things: \Ni a \textit{metric} and \Nii a \textit{dataset}. 

\paragraph{Metrics}
\begin{itemize}
	\item ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
	\item BLEU (BiLingual Evaluation Understudy)
	\item BPC (\eg Perplexity): The bits per character (BPC) evaluation is an example of an entropy-based evaluation for language models. 
\end{itemize}

\paragraph{Industry benchmarks}
\begin{itemize}
	\item GLUE (General Language Understanding Evaluation) is essentially a standardized test for language models to measure performance versus humans and each other on language tasks meant to test understanding. 
	\item SuperGLUE
	\item MMLU (Massive Multitask Language Understanding). 
\end{itemize}

\paragraph{Responsible AI benchmarks}
\begin{itemize}
	\item HONEST evaluation metric compares how hurtful prompt completions are for different genders. 
	\item Some datasets: 
		\begin{itemize}
			\item WinoBias dataset focuses on gender bias. 
			\item CALM
			\item WinoQueer
		\end{itemize}
\end{itemize}

\paragraph{Developing your own benchmark}
\begin{itemize}
	\item \href{https://github.com/openai/evals}{OpenAI's Evals library} 
	\item \href{https://huggingface.co/docs/evaluate/en/index}{Huggingface's Evaluate}
\end{itemize}

\paragraph{Evaluating code generators}

The basic setup looks like this:
\begin{enumerate}
	\item Have your model generate code based on docstrings.
	\item Run the generated code in a safe environment on prebuilt tests to ensure they work and that no errors are thrown
	\item Run the generated code through a profiler and record the time it takes to complete. 
	\item Run the generated code through a security scanner and count the number of vulnerabilities. 
	\item Run the generated code against architectural fitness functions to determine artifacts like how much coupling, integrations, and internal dependencies there are. 
	\item Run steps 1 to 5 on another LLM.
	\item Compare results. 
\end{enumerate}

\paragraph{Evaluating model parameters}

There's a lot you can learn by simply looking at the parameters of an ML model. For instance, an untrained model will have a completely random distribution. 

\begin{lstlisting}[language=Python]
import weightwatcher as ww
from transformers import GPT2Model

gpt2_model = GPT2Model.from_pretrained("gpt2")
gpt2_model.eval()

watcher = ww.WeightWatcher(model=gpt2_model)
details = watcher.analyze(plot=False)
print(details.head())
#    layer_id       name         D  ...      warning        xmax        xmin
# 0         2  Embedding  0.076190  ... over-trained 3837.188332    0.003564
# 1         8     Conv1D  0.060738  ...              2002.124419  108.881419
# 2         9     Conv1D  0.037382  ...               712.127195   46.092445
# 3        14     Conv1D  0.042383  ...              1772.850274   95.358278
# 4        15     Conv1D  0.062197  ...               626.655218   23.727908

\end{lstlisting}
 
The spectral analysis plots evaluate the frequencies of eigenvalues for each layer of a model. These plots tell you whether a model (or layer) looks well-trained and generalizes well or is unstable/poorly conditioned.
Shape of the Spectrum (How eigenvalues are distributed)
\begin{itemize}
	\item Power-law exponent ($\alpha$):
		\begin{itemize}
			\item Good if between 2 and 6: the layer is well-trained.
			\item Bad if $\alpha > 6$: layer might be undertrained or over-regularized.
		\end{itemize}
	\item Fit quality (Dks):
		\begin{itemize}
			\item Low Dks: spectrum matches the expected ``heavy-tailed'' shape, reliable.
			\item High Dks: poor fit, unstable or unstructured layer.
		\end{itemize}
\end{itemize}

\section{Data for LLMs}
It has been shown that data is the most important part of training an LLM. 

% \begin{itemize}
% 	\item WikiText
% 	\item Wiki-40B
% 	\item Europarl
% 	\item Common Crawl
% 	\item OpenWebText
% 	\item The Pile
% 	\item RedPajama
% 	\item OSCAR
% \end{itemize}


\begin{table}[h]
	\setlength{\tabcolsep}{4pt}
	\caption{Summary of datasets}
	\centering
	\begin{tabular}{llrl}
		\toprule
		Dataset & Contents & Size & LastUpdate \\
		\midrule
		WikiText & English Wikipedia & $<$1GB & 2016 \\
		Wiki-40B & Multi-lingual Wikipedia & 10GB & 2020 \\
		Europarl & European Parliament proceedings & 1.5GB & 2011 \\
		Common Crawl & The internet & $\sim$ 300GB & Ongoing \\
		OpenWebText & Curated internet using Reddit & 55GB & 2019 \\
		The Pile & Everything above plus specialty datasets (books, law, med) & 825GB & 2020 \\
		\multirow{2}{*}{RedPajama} & GitHub, arXiv, Books, Wikipedia, StackExchange & 5TB & 2023 \\
								  &, and multiple version of Common Crawl&\\
		OSCAR & Highly curated multilingual dataset with 166 languages & 9.4TB & Ongoing\\
		\bottomrule
	\end{tabular}
\end{table}


\subsection{Data cleaning and preparation}
If you pulled any of the previously mentioned datasets, you might be surprised to realize most of them are just giant text dumps. There are no labels or annotations, and feature engineering hasn't been done at all. 

LLMs are trained via self-supervised manner to predict the next word or a masked workd, so a lot of traditional data cleaning and preparation processes are unneeded. This fact leads many to believe that data cleaning as a whole is unnecessary. 

Data cleaning and curation are difficult, time-consuming, and ultimately subjective tasks that are difficult to tie to key performance indicators (KPIs). Still, taking the time and resources to clean your data will create a more consistent and unparalleled user experience. 

The right frame of mind when preparing your dataset:
\begin{enumerate}
	\item Take your pie of data and determine a schema for the features
	\item Make sure all the features conform to a distribution that makes sense for the outcome your're trying to get through normalization or scaling.
	\item Check the data for bia/anomalies (most businesses skip this step by using automated checking instead of informed verification).
	\item Convert the data into a format for the model to ingest (for LLMs, it's through tokenization and embedding).
	\item Train, check, and retrain.
\end{enumerate}

\begin{commentbox}{Note}
	For more information, check out Fundamentals of Data Engineering, WizardLM, and LIMA: Less Is More for Alignment. 
\end{commentbox}


\paragraph{Instruct Schema} is one of the most effective and widely used data formats for fine-tuning models. Instruction tuning works on the principle that providing a model with explicit instructions for a task leads to better performance than simply giving it raw prompts and answers. In this approach, the data explicitly demonstrates what the model should do, making it clearer and more aligned with human intent. However, preparing such datasets is more demanding than assembling general web data, since each entry must be carefully constructed to match a structured format, typically including an instruction, optional input, and the expected output. You need to prepare your data to match a format that will look something like this:
\begin{lstlisting}[]
###Instruction

{user input}

###Input

{meta info about the instruction}

###Response

{model output}
\end{lstlisting}
It is a structured way of formatting data so that each example clearly contains:
\begin{itemize}
	\item An instruction (what the model should do).
	\item An input (optional context or data the model works on).
	\item An output (the desired response).
\end{itemize}

For instance, 
\begin{lstlisting}
{
  "instruction": "Translate the following English text into Korean.",
  "input": "The stock market saw significant volatility today due to global economic concerns.",
  "output": <Translations>
}
\end{lstlisting}

\begin{commentbox}{Note}
	\begin{itemize}
		\item EvolInstruct: WizardLM
		\item Self-instruct format, Alpaca
	\end{itemize}
\end{commentbox}

\paragraph{Ensuring proficiency with speech acts}
When preparing a dataset for training a model, the most important factor is ensuring the data truly reflects the task you want the model to perform. Misaligned or overly generic data reduces performance and can cause unpredictable behavior.

Dataset alignment:
\begin{itemize}
	\item Training data must match the intended task (e.g., don’t train on Titanic survivors if you want to predict Boston housing prices).
	\item In real-world use cases (like fast-food ordering), interactions are more diverse and unpredictable than generic datasets suggest.
\end{itemize}
Robustness and Risks:
\begin{itemize}
	\item Instruction datasets require intentional design: if a model is only trained on “helpful” responses, it might follow harmful instructions (e.g., “help me take over the world”).
	\item With tool access (Google, HR docs), this becomes even riskier.
\end{itemize}

Understanding speech acts (directives, representatives, commissives, expressives, declarations, verdictives) helps design datasets that match realistic user interactions.
\begin{itemize}
	\item In language learning, this means learners should not only know grammar/vocabulary but also how to perform speech acts appropriately:
		\begin{itemize}
			\item How to make polite requests
			\item How to refuse without sounding rude
			\item How to apologize or thank in culturally acceptable ways
		\end{itemize}
	\item In AI / LLM context, it means training the model to:
		\begin{itemize}
			\item Generate outputs that correctly perform the intended communicative function (e.g., distinguish between an instruction, a suggestion, or a formal declaration).
			\item Handle pragmatic nuances, politeness, indirectness, etc.
		\end{itemize}
\end{itemize}
Speech acts refer to the various functions language can perform in communication beyond conveying information. They are a way of categorizing utterances based on their intended effect or purpose in a conversation. In short, it is an action performed through speaking. For example: 
\begin{itemize}
	\item Assertives $\to$ stating something true/false.
	\item Directives $\to$ requesting, commanding (\eg ``Get it done in the next three days'').
	\item Commissives $\to$ promising, committing (\eg ``I swear'').
	\item Expressives $\to$ greetings, apologizing (\eg ``You are the best'').
	\item Declarations $\to$ enacting something by saying it (\eg ``I now pronounce you married'').
	\item Questions (\eg ``What is this?'')
\end{itemize}

\paragraph{Annotating the data}
Annotation is labeling your data, usually in a positionally aware way. For speech recognition tasks, annotations would identify the different words as noun, verb, adjective, or adverb. Annotations essentially give us metadata that makes it easier to reason about and analyze our datasets. 

There are tools to help with the task:
\begin{itemize}
	\item \href{https://prodi.gy/}{Prodigy}: multimodal annotation tool.
	\item \href{https://github.com/doccano/doccano}{doccano}: Open-source web-based platform for data annotation. 
	\item \href{https://www.fon.hum.uva.nl/praat/}{Praat}: The audio annotation tool.
	\item \href{https://galileo.ai/}{Galileo}: Galileo's LLM studio helps create prompt, evaluate and speed up annotation. 
\end{itemize}

\section{Text Processors}

We need to transform our dataset into something that can be consumed by the LLM. Simply, we need to turn the text into numbers.  

\subsection{Tokenization}
The tokenization is often ignored when working with an LLM through an API, but it is actually vitally important for every subsequent step, and it affects the LLM's performance significantly. 
Tokenization defines how raw text is decomposed into discrete units (tokens), which are then mapped to integer IDs and processed by the model. Because the model operates exclusively on these token sequences, the tokenizer effectively determines what linguistic patterns the model can represent efficiently.
A well-designed tokenizer improves computational efficiency, maximizes effective context utilization, and reduces semantic fragmentation, thereby directly influencing both the quality and reliability of downstream model outputs.

Key concepts:
\begin{itemize}
	\item character vs word vs subword:
		\begin{itemize}
			\item Character tokenizers split into single Unicode codepoints; simple but inefficient for long-range semantics.
			\item Word tokenizers split on whitespace/punctuation; fast but poor for rare words and morphology.
			\item Subword tokenizers (BPE, WordPiece, SentencePiece) balance vocabulary size and OOV(Out of vocabulary) handling and are the most common for modern LLMs.
		\end{itemize}
	\item Special tokens: reserve tokens for padding, beginning/end-of-sequence, unknowns, separators, and task-specific markers. Ensure consistency between tokenizer and model.
	\item Offsets and alignment: maintain offset mappings (token -> character span) when you need to map model outputs back to original text (useful for evaluation, annotation, highlighting).
	\item Token limits and chunking: handle long documents by sliding windows, overlapping chunks, or hierarchical encoders. Decide on truncation strategy (head, tail, or balanced) based on task.
	\item Determinism & reproducibility: fix preprocessing (lowercasing, stripping, normalization), and save tokenizer vocab + merges to reproduce training and inference behavior.
\end{itemize}

Practical tips:
\begin{itemize}
	\item Use a byte-level BPE or SentencePiece for multilingual robustness and reproducibility.
	\item Tokenize and measure token distribution early to size your context windows and batch planning.
	\item Cache tokenized datasets (on disk or in memory) to speed up repeated training/evaluation runs.
	\item When fine-tuning, prefer the tokenizer that matches the pretrained weights to avoid embedding mismatches; if you must extend the vocab, initialize new embeddings carefully and validate downstream performance.
\end{itemize}

Common libraries:
\begin{itemize}
	\item \href{https://github.com/huggingface/tokenizers}{Hugging Face Tokenizers} : A de facto standard tokenizer that offers a full-featured Python implementation for research workflows, alongside a highly optimized Rust implementation for production environments.
	\item \href{https://github.com/google/sentencepiece}{sentencepiece}  : Google's standalone C++ based library for sentencepiece
	\item \href{https://github.com/openai/tiktoken}{tiktoken} : OpenAI's fast and efficient BPE tokenizer for GPT-family models
\end{itemize}


\subsection{Embeddinga}

While tokenization slices the text, embedding constructs the semantic bridge between human language and machine representation. For LLM practitioners, embeddings are not just about representing words; they are the backbone of Retrieval-Augmented Generation (RAG).

In a production environment, simply "turning text into numbers" is insufficient. We must consider the \textbf{architecture of the retrieval pipeline}, \textbf{inference latency}, and \textbf{storage costs}.

\subsubsection{Contextualized Representations}
Unlike static embeddings (e.g., Word2Vec, GloVe) which map a unique token to a fixed vector, modern LLM-based embeddings are \textit{contextual}. The vector representation of a token $t_i$ depends on all other tokens $t_{1...n}$ in the sequence.

\begin{itemize}
    \item \textbf{Pooling Strategies}: To represent a full sentence or document as a single vector (dense vector), we typically extract the output from the final layer of the Transformer.
    \begin{itemize}
        \item \texttt{[CLS] token}: Using the vector of the special classification token (common in BERT-family models).
        \item \texttt{Mean pooling}: Averaging the vectors of all tokens (often yields better semantic representations for clustering).
    \end{itemize}
	\item \textbf{Instruction-Tuned Embeddings}: Modern SOTA models (e.g., E5, Qwen3-Embedding series) require specific instructions or prefixes to distinguish between the query and the document (e.g., adding \texttt{"query: "} or \texttt{"passage: "}).\footnote{Providers often distinguish between symmetric tasks (like "text-matching" or "sentence similarity") and asymmetric tasks ("retrieval"), releasing separate model variants or prompts for each. This addresses the inherent asymmetry in RAG, where user queries are typically short and implicit, while documents are long and noisy.} Omitting these prompts can significantly degrade retrieval performance.

\end{itemize}


\subsubsection{Optimization and Efficiency}
As vector databases grow, storage and latency become bottlenecks. Two techniques are essential for production scaling:

\begin{enumerate}
    \item \textbf{Matryoshka Representation Learning (MRL)}: 
    Newer models (e.g., OpenAI's \texttt{text-embedding-3} or Qwen3-Embedding series) are trained such that information is concentrated in lower-index dimensions. As a result, truncating a 4096-dimensional embedding to 2048 or even 1024 dimensions incurs minimal performance degradation, while significantly reducing storage and retrieval costs.
    
    \item \textbf{Quantization}: 
    Converting 32-bit floating-point vectors (FP32) to 8-bit integers (INT8) or even binary makes. While this introduces a small precision loss, it can reduce memory usage by 4x to 32x and significantly speed up similarity calculations.
\end{enumerate}

\subsubsection{Selecting the Right Model}
Do not blindly use the default model provided by your cloud provider. Consult the 
\item \href{https://huggingface.co/spaces/mteb/leaderboard}{MTEB (Massive Text Embedding Benchmark)} Leaderboard.

\begin{itemize}
    \item \textbf{Task Specificity}: If you are building a semantic search engine, look at the \textit{Retrieval} score. If you are doing intent classification, look at the \textit{Clustering} or \textit{Classification} scores.
    \item \textbf{Sequence Length}: Ensure the model's maximum context length (e.g., 512 vs. 8192 tokens) matches your document chunking strategy. Long-context models are preferred for RAG to capture broader document semantics.
\end{itemize}





