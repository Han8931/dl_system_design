\chapter{LLM Services: A Practical Guide}

\textit{Production} refers to the phase where the model is integrated into live or operational environment to perform its intended tasks or provide services to end users. It's a crucial phase in making the model available for real-world applications and services. In this chapter, we will explore how to package up an LLM into a service or API so that it can take on-demand requests. Then, we will study how to set up a cluster in the cloud where you can deploy this service. 

\section{Creating an LLM service}

\subsection{Model Compilation}
The success of any model in production is dependent on the hardware it runs on. Unfortunately, when programming in a high-level language like Python-based frameworks like PyTorch or TensorFlow, the model won't be optimized to take full advantage of the hardware. This is where compiling comes into play. Compiling is the process of taking code written in a high-level language and converting or lowering it to machine-level code that the computer can process quickly. Compiling your LLm can easily lead to major inference and cost improvements. 

\paragraph{Kernel Tuning} In DL, and high-performance computing, a kernel is a small program or function designed to run on a GPU or other similar processors. These routines are developed by the hardware vendor to maximize chip efficiency. 

During kernel tuning, the most suitable kernels are chosen from a large collection of highly optimized kernels. 


\paragraph{Kernel Fusion}
A \textbf{GPU kernel} is the tiny function that runs on each element. \textbf{Fusion} $=$ do several elementwise ops in one kernel so each element is read from VRAM once, processed in registers, then written back once.

\begin{commentbox}{The \texttt{y = ReLU(x + b)} example}
Assume \texttt{x} and \texttt{b} are the same length (or \texttt{b} is broadcast on the last dim).

Without fusion (two kernels: add, then ReLU):
\begin{enumerate}
	\item For every element i:
	\item Read x[i] from VRAM
	\item Read b[i] from VRAM
	\item Compute t = x[i] + b[i]
	\item Write t to VRAM ← intermediate array
	\item Read t from VRAM
	\item Compute y = max(t, 0)
	\item Write y to VRAM
\end{enumerate}
Global-memory ops per element: 5 (read x, read b, write t, read t, write y)
Two kernel launches (one for add, one for ReLU).

With fusion (one kernel: add+ReLU together)
\begin{enumerate}
	\item For every element i:
	\item Read x[i] from VRAM
	\item Read b[i] from VRAM
	\item Compute t = x[i] + b[i] (kept in a register, not VRAM)
	\item Compute y = max(t, 0)
	\item Write y to VRAM
\end{enumerate}
Global-memory ops per element: 3 (read x, read b, write y)
One kernel launch.
\end{commentbox}

ReLU Example:
\begin{lstlisting}[language=Python]
import torch, time

B, C = 4096, 4096
x = torch.randn(B, C, device="cuda", dtype=torch.float32)
b = torch.randn(C,     device="cuda", dtype=torch.float32)

def add_relu_unfused(x, b):
    # Eager mode typically launches two kernels (add, then relu)
    return torch.relu(x + b)

# PyTorch 2.x compiler (Inductor) generates fused kernels automatically
# This fuses common elementwise chains (like add→ReLU) into single kernels.
add_relu_fused = torch.compile(add_relu_unfused, backend="inductor")

# Correctness check
with torch.no_grad():
    y1 = add_relu_unfused(x, b)
    y2 = add_relu_fused(x, b)
    print("max |diff| =", (y1 - y2).abs().max().item())

# Simple timing
def bench(fn, iters=50, warmup=10):
    for _ in range(warmup):
        fn(x, b); torch.cuda.synchronize()
    t0 = time.perf_counter()
    for _ in range(iters):
        fn(x, b)
    torch.cuda.synchronize()
    return (time.perf_counter() - t0) * 1000 / iters

print("Unfused  :", bench(add_relu_unfused), "ms/iter")
print("Fused    :", bench(add_relu_fused),  "ms/iter")
\end{lstlisting}


\paragraph{Graph Optimization}
Graph optimization = semantics-preserving rewrites of the op graph (fold, fuse, simplify, relayout, retype, reschedule) so the lowered kernels do less work with less memory traffic

\paragraph{TensorRT} NVIDIA TensorRT is an SDK that converts a trained model into an optimized ``engine'' and then runs that engine with very low latency/high throughput on NVIDIA GPUs. It includes an optimizer (compiler) and a lightweight runtime.

How it works (typical workflow):
\begin{enumerate}
	\item Import your model (usually ONNX; there are parsers and framework bridges).
	\item Build an optimized engine: TensorRT selects fast kernels ("tactics"), fuses layers, lowers precision (FP16/INT8) if allowed, and specializes to your shapes/hardware.
	\item Serialize the engine to a .plan file (so you can load it instantly in production).
	\item Run it via the TensorRT runtime (C++/Python).
\end{enumerate}

\begin{lstlisting}[language=Python]
import tensorrt as trt

logger  = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(logger)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser  = trt.OnnxParser(network, logger)

with open("model.onnx", "rb") as f:
    assert parser.parse(f.read()), parser.get_error(0)

config = builder.create_builder_config()
config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 << 30)  # workspace budget
config.set_flag(trt.BuilderFlag.FP16)  # enable mixed precision if supported

# Dynamic-shape profile for input "input" (name must match your ONNX)
profile = builder.create_optimization_profile()
profile.set_shape("input", min=(1,3,224,224), opt=(8,3,224,224), max=(32,3,224,224))
config.add_optimization_profile(profile)

engine = builder.build_engine(network, config)
with open("model.plan", "wb") as f:
    f.write(engine.serialize())
\end{lstlisting}



\paragraph{ONNX Runtime}

\textbf{ONNX}, which stands for Open Neural Network Exchange, is an open source format and ecosystem designed for representing and interoperating between different deep learning frameworks. It was created to address the challenge of model portability and compatibility. ONNX is an IR (intermediate representation) and it allows you to represent models trained in some deep learning framework (\eg TensorFlow, PyTorch) in a standardized format easily consumed by other frameworks and it facilitates the exchange of models between different tools and environments. Unlike TensorRT, ONNX Runtime is intended to be hardware-agnostic, meaning it can be used with a variety of hardware accelerators, including CPUs, GPUs, and specialized hardware like TPUs. 

\subsection{LLM storage strategies}

Now we have a compiled model, we need to think about how our service will access it.This step is critical, because boot times can be a nightmare when working with LLMs since it can take a long time to load such large assets into memory.  

Object storage systems break up assets into small franctional bits called objects. They allow us to federate the entire asset across multiple machines and physical memory locations, a powerful tool that powers the clound, and to cheaply store large objects on hardware.  

\paragraph{Fusing} is the process of mounting a bucket to your machine as if it were an external hard drive. 

\paragraph{Baking the Model}

Baking is the process of putting your model into the Docker image. It is considered an \textit{anti-pattern}. 
\begin{itemize}
	\item Gigantic images: every update means pushing/pulling multi-GB layers → slow CI/CD, slow Kubernetes rollouts, higher storage costs.
	\item Tight coupling: a new model version requires a full image rebuild and redeploy (harder A/B tests, slower rollbacks, no "one image, many models").
	\item Poor caching: one byte change in weights invalidates a huge layer; your build cache won't help much.
	\item Security issue
\end{itemize}

\paragraph{Hybrid: download once, reuse many}

Download the model at boot time but store it in a volume that is mounted at boot time. While this doesn't help at all with the first deployment in a region, it does substantially help any new instances, as they can simply mount this same volume and have the model available to load without having to download. 

\begin{itemize}
	\item At boot (when the service starts up), your service (or an init step) pulls the model from S3/MinIO/HF/etc.
	\item It stores the files on a persistent volume that's mounted into the container.
	\item Subsequent pods/containers on the same node (or across nodes if using a shared RWX volume) just mount the volume—no re-download.
	\item You keep your app image small and decouple model updates from image builds.
\end{itemize}


\subsection{Adaptive request batching}

A typical API will accept and process requests in the order they are received, processing them immediately and as quickly as possible. However, anyone who's trained a ML model has come to realize that there are mathematical and computational advantages to running inference in batches of powers of 2 (16, 32, 64, etc), particularly when GPUs are involved, where we can take advantage of better memory alignment or vectorized instructions parallelizing computations across the GPU cores.  

\paragraph{Why power of 2 is better}

Reference: \href{https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html}{CUDA C++ Best Practices Guide}

https://datascience.stackexchange.com/questions/20179/what-is-the-advantage-of-keeping-batch-size-a-power-of-2
% \begin{enumerate}
% 	\item Computers do work in fixed chunks: GPUs (and CPUs) prefer doing work in evenly sized blocks. When your numbers line up with those blocks, nothing is wasted.
% 	\item GPU workers come in packs (warps): On NVIDIA, the scheduler runs threads in groups of 32 (a "warp", is the scheduling unit of 32 threads).
% 		\begin{itemize}
% 			\item If your batch size is 32, every seat is filled.
% 			\item If it's 30, you still launch a warp of 32, but 2 threads sit
% 				idle $\to$ tiny waste, repeated many times.
% 		\end{itemize}
% 	\item Math kernels are cut into tiles (often 8 or 16): Matrix multiplies and attention ops slice work into tiles like 16×16.
% 		\begin{itemize}
% 			\item If your dimensions (batch, sequence, hidden) are multiples
% 				of 16, every tile is full. 
% 			\item If not, the edge tiles are partially filled and run
% 				slower "remainder" code.
% 		\end{itemize}
% 	\item Memory is fetched in aligned blocks
% 		\begin{itemize}
% 			\item GPUs pull data in aligned chunks (e.g., 128/256 bytes).
% 			\item When sizes align (multiples of 8/16/32), you get fewer,
% 				wider memory transactions $\to$ less time waiting on memory.
% 		\end{itemize}
% 	\item Powers of 2 happen to fit all the above
% 		\begin{itemize}
% 			\item Numbers like 16, 32, 64 are divisible by 2, 4, 8, 16 and so on. That means they usually:
% 			\item  fill warps (32),
% 			\item  fill tiles (8/16),
% 			\item  align memory nicely. So they're a safe default that often unlocks the fastest code paths. 
% 		\end{itemize}
% \end{enumerate}

What adaptive batching does is essentially pool requests together over a certain period of time. Once the pool receives the configured maximum batch size or the timer runs out, it will run inference on the entire batch through the model, sending the results back to the individual clients that requested them. Essentially, it's a queue. Setting one up yourself can and will be a huge pain; thankfully, most ML inference services offer this out of the box, and almost all are easy to implement. For example, in \textbf{BentoML}, add \texttt{@bentoml.Runnable.method(batchable=True)} as a decorator to your predict function, and in \textbf{Triton Inference Server}, add \texttt{dynamic\_batching\{\}} at the end of your model definition file.

If that sounds easy, it is. Typically, you don't need to do any further finessing, as the defaults tend to be very practical. That said, if you are looking to maximize every bit of efficiency possible in the system, you can often set a maximum batch size, which will tell the batcher to run once this limit is reached, or a batch delay, which does the same thing but for the timer. Increasing either will result in longer latency but likely better throughput, so typically these are only adjusted when your system has plenty of latency budget.

Overall, the benefits of adaptive batching include better use of resources and higher throughput at the cost of a bit of latency. This is a valuable trade-off, and we recommend giving your product the latency bandwidth to include this feature. In our experience, optimizing for throughput leads to better reliability and scalability and thus greater customer satisfaction. Of course, when latency times are extremely important or traffic is few and far between, you may rightly forgo this feature.

\subsection{Flow Control}
Rate limiters and access keys are critical protections for an API, especially one sitting in front of an expensive LLM. Rate limiters control the number of requests a client can make to an API within a specified time, which helps protect the API server from abuse, such as distributed denial of service (DDoS) attacks, where an attacker makes numerous requests simultaneously to overwhelm the system and hinder its function.  

Rate limiters can also protect the server from bots that make numerous automated requests in a short span of time. This helps manage the server resources optimally so the server is not exhausted due to unnecessary or harmful traffic. They are also useful for managing quotas, thus ensuring all users have fair and equal access to the API's resources. By preventing any single user from using excessive resources, the rate limiter ensures the system functions smoothly for all users. 

All in all, rate limiters are an important mechanism for controlling the flow of your LLM's system processes. They can play a critical role in dampening bursty workloads and preventing your system from getting overwhelmed during autoscaling and rolling updates, especially when you have a rather large LLM with longer deployment times. Rate limiters can take several forms, and the one you choose will be dependent on your use case. 

\begin{commentbox}{Types of rate limiters}
\begin{itemize}
	\item Fixed window
	\item Sliding window log
	\item Token bucket
	\item Leaky bucket
\end{itemize}
\end{commentbox}

A rate limiter can be applied at multiple levels, from the entire API to individual client requests to specific function calls. 

\begin{lstlisting}[language=Python]
from typing import Optional

from fastapi import FastAPI, Depends, HTTPException, status, Request
from fastapi.security import APIKeyHeader
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.errors import RateLimitExceeded
from slowapi.util import get_remote_address

# ----- Config ---------------------------------------------------------------
# In real apps, load from a DB or env/secret manager.
VALID_KEYS = {"1234567abcdefg"}

# Displayed in Swagger UI as the auth "scheme" name
api_key_header = APIKeyHeader(name="X-API-Key", scheme_name="APIKey", auto_error=False)

# Prefer per-key limiting; fall back to client IP (useful when no key provided)
def rate_key(request: Request) -> str:
    key = request.headers.get("X-API-Key")
    return f"key:{key}" if key else f"ip:{request.client.host}"

# Use in-memory storage for single-process dev.
# For multi-workers/replicas, switch to: storage_uri="redis://redis:6379/0"
limiter = Limiter(key_func=rate_key, storage_uri="memory://")

# ----- App setup ------------------------------------------------------------
app = FastAPI(title="API-Key + Rate Limit Example")
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# If running behind a reverse proxy/load balancer, trust X-Forwarded-*.
# In production, replace ["*"] with your proxy host/IP(s).

# ----- Auth dependency ------------------------------------------------------
async def require_api_key(api_key: Optional[str] = Depends(api_key_header)) -> str:
    """
    Validates the X-API-Key header against our allow-list.
    Returns the key (or user id associated with it) for downstream use.
    """
    if not api_key or api_key not in VALID_KEYS:
        # RFC-friendly header (helps some clients know how to auth)
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid or missing API key",
            headers={"WWW-Authenticate": "APIKey"},
        )
    return api_key

# ----- Routes ---------------------------------------------------------------
@app.get("/health", include_in_schema=False)
async def health() -> dict:
    return {"ok": True}

@app.get("/hello")
@limiter.limit("5/minute")            # per-key if present; else per-IP
async def hello(request: Request, api_key: str = Depends(require_api_key)):
    # SlowAPI needs `request`; dependency injects validated `api_key`.
    return {"message": "Hello World"}

# Optional: run directly with `python main.py`
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="127.0.0.1", port=8000, reload=True)
\end{lstlisting}
\begin{itemize}
	\item You (the API owner) issue a unique key (\ie \texttt{VALID\_KEYS}) to each user/app.
	\item The client keeps that key and sends it with every request.
	\item Because the client passes the key in an HTTP header. Your example uses an \texttt{X-API-Key} header via \texttt{APIKeyHeader}.
	\item The dependency \texttt{require\_api\_key(...)} reads the header, checks it against \texttt{VALID\_KEYS}, and returns 401 if it's missing/invalid.
	\item The rate limiter's \texttt{key\_func} (\texttt{rate\_key}) also reads the same header to bucket requests per key.
\end{itemize}
To test in SwaggerUI:
\begin{itemize}
	\item Click Authorize.
	\item You'll see a field for APIKey (because APIKeyHeader is used).
	\item Paste ``1234567abcdefg'' and authorize
\end{itemize}

\subsection{Streaming responses}

\subsection{Feature store}
When it comes to running ML models in production, feature stores really simplify the inference process. We first introduced these in chapter 3, but as a recap, feature stores establish a centralized source of truth. They answer crucial questions about your data:

Who is responsible for the feature? What is its definition? Who can access it? Let's take a look at setting one up and querying the data to get a feel for how they work. We'll be using Feast, which is open source and supports a variety of backends. To get started, let us pip install feast and then run the init command in your terminal to set up a project, like so:

The app we are building is a question-and-answer service. Q\&A services can greatly benefit from a feature store's data governance tooling. For example, point-in-time joins help us answer questions like "Who is the president of x?" where the answer is expected to change over time. Instead of querying just the question, we query the question with a timestamp, and the point-in-time join will return whatever the answer to the question was in our database at that point in time. In the next listing, we pull a Q\&A dataset and store it in a parquet format in the data directory of our Feast project.

\subsection{Retrieval-augmented generation}
\subsection{LLM service libraries}

\section{Setting up infrastructure}
\subsection{Provisioning clusters}
