\chapter{LLM Services: A Practical Guide}

\textit{Production} refers to the phase where the model is integrated into live or operational environment to perform its intended tasks or provide services to end users. It's a crucial phase in making the model available for real-world applications and services. In this chapter, we will explore how to package up an LLM into a service or API so that it can take on-demand requests. Then, we will study how to set up a cluster in the cloud where you can deploy this service. 

\section{Creating an LLM service}

\subsection{Model Compilation}
The success of any model in production is dependent on the hardware it runs on. Unfortunately, when programming in a high-level language like Python-based frameworks like PyTorch or TensorFlow, the model won't be optimized to take full advantage of the hardware. This is where compiling comes into play. Compiling is the process of taking code written in a high-level language and converting or lowering it to machine-level code that the computer can process quickly. Compiling your LLm can easily lead to major inference and cost improvements. 

\paragraph{Kernel Tuning} In DL, and high-performance computing, a kernel is a small program or function designed to run on a GPU or other similar processors. These routines are developed by the hardware vendor to maximize chip efficiency. 

During kernel tuning, the most suitable kernels are chosen from a large collection of highly optimized kernels. 


\paragraph{Kernel Fusion}
A GPU kernel is the tiny function that runs on each element. \textbf{Fusion} $=$ do several elementwise ops in one kernel so each element is read from VRAM \footnote{VRAM is technically a type of DRAM, but it is optimized for storing images and video data that the GPU (Graphics Processing Unit) requires for processing various graphics. It is primarily responsible for storing textures, frame buffers, image data, video data, and other graphics-related data \cite{Anshul2024}.} once, processed in registers, then written back once.

\begin{commentbox}{The \texttt{y = ReLU(x + b)} example}
Assume \texttt{x} and \texttt{b} are the same length (or \texttt{b} is broadcast on the last dim).

Without fusion (two kernels: add, then ReLU):
\begin{enumerate}
	\item For every element i:
	\item Read x[i] from VRAM
	\item Read b[i] from VRAM
	\item Compute t = x[i] + b[i]
	\item Write t to VRAM ← intermediate array
	\item Read t from VRAM
	\item Compute y = max(t, 0)
	\item Write y to VRAM
\end{enumerate}
Global-memory ops per element: 5 (read x, read b, write t, read t, write y)
Two kernel launches (one for add, one for ReLU).

With fusion (one kernel: add+ReLU together)
\begin{enumerate}
	\item For every element i:
	\item Read x[i] from VRAM
	\item Read b[i] from VRAM
	\item Compute t = x[i] + b[i] (kept in a register, not VRAM)
	\item Compute y = max(t, 0)
	\item Write y to VRAM
\end{enumerate}
Global-memory ops per element: 3 (read x, read b, write y)
One kernel launch.
\end{commentbox}

ReLU Example:
\begin{lstlisting}[language=Python]
import torch, time

B, C = 4096, 4096
x = torch.randn(B, C, device="cuda", dtype=torch.float32)
b = torch.randn(C,     device="cuda", dtype=torch.float32)

def add_relu_unfused(x, b):
    # Eager mode typically launches two kernels (add, then relu)
    return torch.relu(x + b)

# PyTorch 2.x compiler (Inductor) generates fused kernels automatically
# This fuses common elementwise chains (like add→ReLU) into single kernels.
add_relu_fused = torch.compile(add_relu_unfused, backend="inductor")

# Correctness check
with torch.no_grad():
    y1 = add_relu_unfused(x, b)
    y2 = add_relu_fused(x, b)
    print("max |diff| =", (y1 - y2).abs().max().item())

# Simple timing
def bench(fn, iters=50, warmup=10):
    for _ in range(warmup):
        fn(x, b); torch.cuda.synchronize()
    t0 = time.perf_counter()
    for _ in range(iters):
        fn(x, b)
    torch.cuda.synchronize()
    return (time.perf_counter() - t0) * 1000 / iters

print("Unfused  :", bench(add_relu_unfused), "ms/iter")
print("Fused    :", bench(add_relu_fused),  "ms/iter")
\end{lstlisting}


\paragraph{Graph Optimization}
Graph optimization = semantics-preserving rewrites of the op graph (fold, fuse, simplify, relayout, retype, reschedule) so the lowered kernels do less work with less memory traffic

\paragraph{TensorRT} NVIDIA TensorRT is an SDK that converts a trained model into an optimized ``engine'' and then runs that engine with very low latency/high throughput on NVIDIA GPUs. It includes an optimizer (compiler) and a lightweight runtime.

How it works (typical workflow):
\begin{enumerate}
	\item Import your model (usually ONNX; there are parsers and framework bridges).
	\item Build an optimized engine: TensorRT selects fast kernels ("tactics"), fuses layers, lowers precision (FP16/INT8) if allowed, and specializes to your shapes/hardware.
	\item Serialize the engine to a .plan file (so you can load it instantly in production).
	\item Run it via the TensorRT runtime (C++/Python).
\end{enumerate}

\begin{lstlisting}[language=Python]
import tensorrt as trt

logger  = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(logger)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser  = trt.OnnxParser(network, logger)

with open("model.onnx", "rb") as f:
    assert parser.parse(f.read()), parser.get_error(0)

config = builder.create_builder_config()
config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 << 30)  # workspace budget
config.set_flag(trt.BuilderFlag.FP16)  # enable mixed precision if supported

# Dynamic-shape profile for input "input" (name must match your ONNX)
profile = builder.create_optimization_profile()
profile.set_shape("input", min=(1,3,224,224), opt=(8,3,224,224), max=(32,3,224,224))
config.add_optimization_profile(profile)

engine = builder.build_engine(network, config)
with open("model.plan", "wb") as f:
    f.write(engine.serialize())
\end{lstlisting}



\paragraph{ONNX Runtime}

\textbf{ONNX}, which stands for Open Neural Network Exchange, is an open source format and ecosystem designed for representing and interoperating between different deep learning frameworks. It was created to address the challenge of model portability and compatibility. ONNX is an IR (intermediate representation) and it allows you to represent models trained in some deep learning framework (\eg TensorFlow, PyTorch) in a standardized format easily consumed by other frameworks and it facilitates the exchange of models between different tools and environments. Unlike TensorRT, ONNX Runtime is intended to be hardware-agnostic, meaning it can be used with a variety of hardware accelerators, including CPUs, GPUs, and specialized hardware like TPUs. 

\subsection{LLM storage strategies}

Now we have a compiled model, we need to think about how our service will access it.This step is critical, because boot times can be a nightmare when working with LLMs since it can take a long time to load such large assets into memory.  

Object storage systems break up assets into small franctional bits called objects. They allow us to federate the entire asset across multiple machines and physical memory locations, a powerful tool that powers the clound, and to cheaply store large objects on hardware.  

\paragraph{Fusing} is the process of mounting a bucket to your machine as if it were an external hard drive. 

\paragraph{Baking the Model}

Baking is the process of putting your model into the Docker image. It is considered an \textit{anti-pattern}. 
\begin{itemize}
	\item Gigantic images: every update means pushing/pulling multi-GB layers → slow CI/CD, slow Kubernetes rollouts, higher storage costs.
	\item Tight coupling: a new model version requires a full image rebuild and redeploy (harder A/B tests, slower rollbacks, no "one image, many models").
	\item Poor caching: one byte change in weights invalidates a huge layer; your build cache won't help much.
	\item Security issue
\end{itemize}

\paragraph{Hybrid: download once, reuse many}

Download the model at boot time but store it in a volume that is mounted at boot time. While this doesn't help at all with the first deployment in a region, it does substantially help any new instances, as they can simply mount this same volume and have the model available to load without having to download. 

\begin{itemize}
	\item At boot (when the service starts up), your service (or an init step) pulls the model from S3/MinIO/HF/etc.
	\item It stores the files on a persistent volume that's mounted into the container.
	\item Subsequent pods/containers on the same node (or across nodes if using a shared RWX volume) just mount the volume—no re-download.
	\item You keep your app image small and decouple model updates from image builds.
\end{itemize}


\subsection{Adaptive request batching}

A typical API will accept and process requests in the order they are received, processing them immediately and as quickly as possible. However, anyone who's trained a ML model has come to realize that there are mathematical and computational advantages to running inference in batches of powers of 2 (16, 32, 64, etc), particularly when GPUs are involved, where we can take advantage of better memory alignment or vectorized instructions parallelizing computations across the GPU cores.  

\paragraph{Why power of 2 is better}

Reference: \href{https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html}{CUDA C++ Best Practices Guide}

https://datascience.stackexchange.com/questions/20179/what-is-the-advantage-of-keeping-batch-size-a-power-of-2
% \begin{enumerate}
% 	\item Computers do work in fixed chunks: GPUs (and CPUs) prefer doing work in evenly sized blocks. When your numbers line up with those blocks, nothing is wasted.
% 	\item GPU workers come in packs (warps): On NVIDIA, the scheduler runs threads in groups of 32 (a "warp", is the scheduling unit of 32 threads).
% 		\begin{itemize}
% 			\item If your batch size is 32, every seat is filled.
% 			\item If it's 30, you still launch a warp of 32, but 2 threads sit
% 				idle $\to$ tiny waste, repeated many times.
% 		\end{itemize}
% 	\item Math kernels are cut into tiles (often 8 or 16): Matrix multiplies and attention ops slice work into tiles like 16×16.
% 		\begin{itemize}
% 			\item If your dimensions (batch, sequence, hidden) are multiples
% 				of 16, every tile is full. 
% 			\item If not, the edge tiles are partially filled and run
% 				slower "remainder" code.
% 		\end{itemize}
% 	\item Memory is fetched in aligned blocks
% 		\begin{itemize}
% 			\item GPUs pull data in aligned chunks (e.g., 128/256 bytes).
% 			\item When sizes align (multiples of 8/16/32), you get fewer,
% 				wider memory transactions $\to$ less time waiting on memory.
% 		\end{itemize}
% 	\item Powers of 2 happen to fit all the above
% 		\begin{itemize}
% 			\item Numbers like 16, 32, 64 are divisible by 2, 4, 8, 16 and so on. That means they usually:
% 			\item  fill warps (32),
% 			\item  fill tiles (8/16),
% 			\item  align memory nicely. So they're a safe default that often unlocks the fastest code paths. 
% 		\end{itemize}
% \end{enumerate}

What adaptive batching does is essentially pool requests together over a certain period of time. Once the pool receives the configured maximum batch size or the timer runs out, it will run inference on the entire batch through the model, sending the results back to the individual clients that requested them. Essentially, it's a queue. Setting one up yourself can and will be a huge pain; thankfully, most ML inference services offer this out of the box, and almost all are easy to implement. For example, in \textbf{BentoML}, add \texttt{@bentoml.Runnable.method(batchable=True)} as a decorator to your predict function, and in \textbf{Triton Inference Server}, add \texttt{dynamic\_batching\{\}} at the end of your model definition file.

If that sounds easy, it is. Typically, you don't need to do any further finessing, as the defaults tend to be very practical. That said, if you are looking to maximize every bit of efficiency possible in the system, you can often set a maximum batch size, which will tell the batcher to run once this limit is reached, or a batch delay, which does the same thing but for the timer. Increasing either will result in longer latency but likely better throughput, so typically these are only adjusted when your system has plenty of latency budget.

Overall, the benefits of adaptive batching include better use of resources and higher throughput at the cost of a bit of latency. This is a valuable trade-off, and we recommend giving your product the latency bandwidth to include this feature. In our experience, optimizing for throughput leads to better reliability and scalability and thus greater customer satisfaction. Of course, when latency times are extremely important or traffic is few and far between, you may rightly forgo this feature.

\subsection{Flow Control}
Rate limiters and access keys are critical protections for an API, especially one sitting in front of an expensive LLM. Rate limiters control the number of requests a client can make to an API within a specified time, which helps protect the API server from abuse, such as distributed denial of service (DDoS) attacks, where an attacker makes numerous requests simultaneously to overwhelm the system and hinder its function.  

Rate limiters can also protect the server from bots that make numerous automated requests in a short span of time. This helps manage the server resources optimally so the server is not exhausted due to unnecessary or harmful traffic. They are also useful for managing quotas, thus ensuring all users have fair and equal access to the API's resources. By preventing any single user from using excessive resources, the rate limiter ensures the system functions smoothly for all users. 

All in all, rate limiters are an important mechanism for controlling the flow of your LLM's system processes. They can play a critical role in dampening bursty workloads and preventing your system from getting overwhelmed during autoscaling and rolling updates, especially when you have a rather large LLM with longer deployment times. Rate limiters can take several forms, and the one you choose will be dependent on your use case. 

\begin{commentbox}{Types of rate limiters}
\begin{itemize}
	\item Fixed window
	\item Sliding window log
	\item Token bucket
	\item Leaky bucket
\end{itemize}
\end{commentbox}

A rate limiter can be applied at multiple levels, from the entire API to individual client requests to specific function calls. 

\begin{lstlisting}[language=Python]
from typing import Optional

from fastapi import FastAPI, Depends, HTTPException, status, Request
from fastapi.security import APIKeyHeader
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.errors import RateLimitExceeded
from slowapi.util import get_remote_address

# ----- Config ---------------------------------------------------------------
# In real apps, load from a DB or env/secret manager.
VALID_KEYS = {"1234567abcdefg"}

# Displayed in Swagger UI as the auth "scheme" name
api_key_header = APIKeyHeader(name="X-API-Key", scheme_name="APIKey", auto_error=False)

# Prefer per-key limiting; fall back to client IP (useful when no key provided)
def rate_key(request: Request) -> str:
    key = request.headers.get("X-API-Key")
    return f"key:{key}" if key else f"ip:{request.client.host}"

# Use in-memory storage for single-process dev.
# For multi-workers/replicas, switch to: storage_uri="redis://redis:6379/0"
limiter = Limiter(key_func=rate_key, storage_uri="memory://")

# ----- App setup ------------------------------------------------------------
app = FastAPI(title="API-Key + Rate Limit Example")
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# If running behind a reverse proxy/load balancer, trust X-Forwarded-*.
# In production, replace ["*"] with your proxy host/IP(s).

# ----- Auth dependency ------------------------------------------------------
async def require_api_key(api_key: Optional[str] = Depends(api_key_header)) -> str:
    """
    Validates the X-API-Key header against our allow-list.
    Returns the key (or user id associated with it) for downstream use.
    """
    if not api_key or api_key not in VALID_KEYS:
        # RFC-friendly header (helps some clients know how to auth)
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid or missing API key",
            headers={"WWW-Authenticate": "APIKey"},
        )
    return api_key

# ----- Routes ---------------------------------------------------------------
@app.get("/health", include_in_schema=False)
async def health() -> dict:
    return {"ok": True}

@app.get("/hello")
@limiter.limit("5/minute")            # per-key if present; else per-IP
async def hello(request: Request, api_key: str = Depends(require_api_key)):
    # SlowAPI needs `request`; dependency injects validated `api_key`.
    return {"message": "Hello World"}

# Optional: run directly with `python main.py`
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="127.0.0.1", port=8000, reload=True)
\end{lstlisting}
\begin{itemize}
	\item You (the API owner) issue a unique key (\ie \texttt{VALID\_KEYS}) to each user/app.
	\item The client keeps that key and sends it with every request.
	\item Because the client passes the key in an HTTP header. Your example uses an \texttt{X-API-Key} header via \texttt{APIKeyHeader}.
	\item The dependency \texttt{require\_api\_key(...)} reads the header, checks it against \texttt{VALID\_KEYS}, and returns 401 if it's missing/invalid.
	\item The rate limiter's \texttt{key\_func} (\texttt{rate\_key}) also reads the same header to bucket requests per key.
\end{itemize}
To test in SwaggerUI:
\begin{itemize}
	\item Click Authorize.
	\item You'll see a field for APIKey (because APIKeyHeader is used).
	\item Paste ``1234567abcdefg'' and authorize
\end{itemize}

\paragraph{LiteLLM} LLM proxy server


\subsection{Streaming responses}

\subsection{Feature store}
When it comes to running ML models in production, feature stores really simplify the inference process. We first introduced these in chapter 3, but as a recap, feature stores establish a centralized source of truth. They answer crucial questions about your data:

Who is responsible for the feature? What is its definition? Who can access it? Let's take a look at setting one up and querying the data to get a feel for how they work. We'll be using Feast, which is open source and supports a variety of backends. To get started, let us pip install feast and then run the init command in your terminal to set up a project, like so:

The app we are building is a question-and-answer service. Q\&A services can greatly benefit from a feature store's data governance tooling. For example, point-in-time joins help us answer questions like "Who is the president of x?" where the answer is expected to change over time. Instead of querying just the question, we \textit{query the question with a timestamp}, and the point-in-time join will return whatever the answer to the question was in our database at that point in time. In the next listing, we pull a Q\&A dataset and store it in a parquet format in the data directory of our Feast project.
\begin{lstlisting}[language=Python]
import pandas as pd
from datasets import load_dataset
import datetime
 
from sentence_transformers import SentenceTransformer
model = SentenceTransformer("all-MiniLM-L6-v2")
 
def save_qa_to_parquet(path):
    squad = load_dataset("squad", split="train[:5000]")   
    ids = squad["id"]                            
    questions = squad["question"]
    answers = [answer["text"][0] for answer in squad["answers"]]
    qa = pd.DataFrame(          
        zip(ids, questions, answers),
        columns=["question_id", "questions", "answers"],
    )
    qa["embeddings"] = model.encode(questions) # feature
    qa["created"] = utcnow() # ingest time
    qa["datetime"] = qa["created"].dt.floor("h") # Event time (when the new fact becomes true)
    qa.to_parquet(path) # offline store file
 
if __name__ == "__main__":
    path = "./data/qa.parquet"
    save_qa_to_parquet(path)
\end{lstlisting}
\begin{itemize}
	\item \texttt{DataFrame.to\_parquet} in pandas is a method used to write a pandas DataFrame to the binary Parquet file format.
		\begin{itemize}
			\item Parquet is a columnar storage format optimized for efficient data storage and retrieval, especially in big data environments.
		\end{itemize}
	\item Creating a tiny feature table with:
		\begin{itemize}
			\item Entity key: question\_id
			\item Features: embeddings (and you could treat answers, questions as features for the demo)
			\item Time columns: datetime (event time), created (ingest time)
		\end{itemize}
		
\end{itemize}

\begin{lstlisting}[language=Python]
from feast import Entity, FeatureView, Field, FileSource, ValueType
from feast.types import Array, Float32, String
from datetime import timedelta
 
path = "./data/qa.parquet"
 
question = Entity(name="question_id", value_type=ValueType.STRING)
 
question_feature = Field(name="questions", dtype=String)
 
answer_feature = Field(name="answers", dtype=String)
 
embedding_feature = Field(name="embeddings", dtype=Array(Float32))
 
questions_view = FeatureView(
    name="qa",
    entities=[question],
    ttl=timedelta(days=1),
    schema=[question_feature, answer_feature, embedding_feature],
    source=FileSource(
        path=path,
        event_timestamp_column="datetime",
        created_timestamp_column="created",
        timestamp_field="datetime",
    ),
    tags={},
    online=True,
)
\end{lstlisting}

\subsection{Retrieval-augmented generation}

\subsection{LLM service libraries}
If you are starting to feel a bit overwhelmed about all the tooling and features you need to implement to create an LLM service, there are several libraries aim to do all of this for you! Some open source libraries of note are \textbf{vLLM} and \textbf{OpenLLM} (by BentoML).

Most of these toolings are still relatively new and under active development, and they are far from feature parity with each other, so pay attention to what they offer.

\section{Setting up infrastructure}

Setting up infrastructure is a critical aspect of modern software development, and we shouldn't expect machine learning to be any different. To ensure scalability, reliability, and efficient deployment of our applications, we need to plan a robust infrastruc- ture that can handle the demands of a growing user base. This is where \textbf{Kubernetes} comes into play.

\textit{Kubernetes, often referred to as k8s, is an open source container orchestration platform that helps automate and manage the deployment, scaling, and management of containerized applications}. It is designed to simplify the process of running and coordinating multiple containers across a cluster of servers, making it easier to scale applications and ensure high availability. We are going to talk a lot about k8s in this chapter, and while you don't need to be an expert, it will be useful to cover some basics to ensure we are all on the same page.

At its core, k8s works by grouping containers into logical units called \textit{pods}, which are the smallest deployable units in the k8s ecosystem. These pods are then scheduled and managed by the k8s control plane (\ie the brain of Kubernetes), which oversees their deployment, scaling, and updates. This control plane consists of several components that collectively handle the orchestration and management of containers. 

\begin{itemize}
	\item Control Plane
		\begin{itemize}
			\item API server: this is the gate to talk to k8s. 
			\item Scheduler: Decides which worker node will run your pod.
			\item Controller manager: Watches everything and fixes it if it drifts from your request.
		\end{itemize}
	\item Worker Nodes: each node is a computer (physical or virtual). It runs:
		\begin{itemize}
			\item Kubelet: The assistant on each node. This talks to the control plane and makes sure the right containers (\ie pod) are running. 
		\end{itemize}
\end{itemize}

How it works?
\begin{itemize}
	\item You tell Kubernetes: ``Run 3 pods of my web app.''
	\item API Server receives the request.
	\item Scheduler decides where to place pods.
	\item Controller Manager keeps checking until 3 pods are alive.
	\item Kubelet on each worker starts the containers.
	\item Kube-Proxy sets up networking so users can reach your app.
\end{itemize}

\subsection{Provisioning clusters}

The first thing to do when starting any project is to set up a \textit{cluster}. A cluster is a collective of worker machines or nodes where we will host our applications. 

\begin{itemize}
	\item On GCP: 
\begin{lstlisting}[language=bash]
gcloud container clusters create <NAME>
\end{lstlisting}
	\item On AWS
\begin{lstlisting}[language=bash]
eksctl create cluster
\end{lstlisting}
\end{itemize}


\begin{itemize}
	\item Provisioning a cluster is like building an ``empty factory'': control room (control plane), power, network, and a small set of machines (nodes) to start with. In short, it creates and prepares the cluster.
	\item Node Auto-Provisioning (NAP) is an auto-hire manager that watches your job queue (unschedulable Pods) and, when there aren't enough machines, buys and installs the right new machines automatically—and removes them later when work slows down. In short, it dynamically adds new nodes/pools to an existing cluster (not create the cluster itself).
\end{itemize}

\subsection{Autoscaling}

One of the big selling points to setting up a k8s cluster is \textit{autoscaling}. Autoscaling is an important ingredient in creating robust production-grade services. The main reason is that \textbf{we never expect any service to receive static request volume}.

\begin{commentbox}{Autoscaling and NAP}
	Autoscaling can be considered as a family of mechanisms at different layers, and NAP is one specific (infrastructure) member of that family.
\end{commentbox}

The HPA (horizontal pod autoscaler) watches CPU and memory resources and will tell the deployment service to increase or decrease the number of replicas. 

The first service we'll need is one that can \textbf{collect the GPU metrics}. From this, we have \textit{NVIDIA's Data Center GPU Manager (DCGM)}, which provides a metrics exporter that can export GPU metrics. DCGM exposes a host of GPU metrics, including temperature and power usage, which can create some fun dashboards, but the most useful metrics for autoscaling are utilization and memory utilization. 

From here, the data will go to a service like \textit{Prometheus}. Prometheus is an open source monitoring system used to monitor Kubernetes clusters and the applications running on them. Prometheus collects metrics from various sources and stores them in a time-series database, where they can be analyzed and queried. Prometheus can collect metrics directly from Kubernetes APIs and from applications running on the cluster using a variety of collection mechanisms such as exporters, agents, and sidecar containers. It's essentially an aggregator of services like DCGM, including features like alerting and notification. It also exposes an HTTP API for service for external tooling like Grafana to query and create graphs and dashboards with.

While Prometheus provides a way to store metrics and monitor our service, the metrics aren't exposed to the internals of Kubernetes. For an HPA to gain access, we will need to register yet another service to either the custom metrics API or external metrics API. By default, Kubernetes comes with the \textit{metrics.k8s.io} endpoint that exposes resource metrics, CPU, and memory utilization. To accommodate the need to scale deployments and pods on custom metrics, two additional APIs were introduced: \textit{custom.metrics.k9s.io} and \textit{external.metrics.k8s.io}. There are some limitations to this setup, as currently, only one ``adapter'' API service can be registered at a time for either one. This limitation mostly becomes a problem if you ever decide to change this endpoint from one provider to another.

For this service, Prometheus provides the \textit{Prometheus Adapter}, which works well, but from our experience, it wasn't designed for production workloads. Alternatively, we would recommend \textit{KEDA}. KEDA (Kubernetes Event-Driven Autoscaling) is an open source project that provides event-driven autoscaling for Kubernetes. It offers more flexibility in terms of the types of custom metrics that can be used for autoscaling.

While Prometheus Adapter requires configuring metrics inside a ConfigMap, any metric already exposed through the Prometheus API can be used in KEDA, providing a more streamlined and friendly user experience. It also offers scaling to and from 0, which isn't available through HPAs, allowing you to turn off a service completely if there is no traffic. That said, you can't scale from 0 on resource metrics like CPU and memory and, by extension, GPU metrics, but it is useful when you are using traffic metrics or a queue to scale.

Putting this all together, you'll end up with the architecture shown in figure 6.7. Compared to figure 6.6, you'll notice at the bottom that DCGM is managing our GPU metrics and feeding them into Prometheus Operator. From Prometheus, we can set up external dashboards with tools like Grafana. Internal to k8s, we'll use KEDA to set up a custom.metrics.k9s.io API to return these metrics so we can autoscale based on the GPU metrics. KEDA has several CRDs, one of which is a ScaledObject, which creates the HPA and provides the additional features.

While autoscaling provides many benefits, it's important to be aware of its limitations and potential problems, which are only exacerbated by LLM inference services. Proper configuration of the HPA is often an afterthought for many applications, but it becomes mission-critical when dealing with LLMs. LLMs take longer to become fully operational, as the GPUs need to be initialized and model weights loaded into memory; these aren't services that can turn on a dime, which often can cause problems when scaling up if not properly prepared for. Additionally, if the system scales down too aggressively, it may result in instances being terminated before completing their assigned tasks, leading to data loss or other problems. Lastly, flapping is just such a concern that can arise from incorrect autoscaling configurations. Flapping happens when the number of replicas keeps oscillating, booting up a new service only to terminate it before it can serve any inferences.

\paragraph{Summary} You want Kubernetes to \textbf{add or remove Pods (and maybe nodes) based on GPU load} (\eg how busy the GPU is, how much GPU memory is used).
\begin{itemize}
	\item DCGM (NVIDIA): the thermometer for your GPUs. It reads GPU stats (utilization, memory used, temperature, power) and exposes them as metrics.
	\item Prometheus: the notebook + calculator. It scrapes (collects) those DCGM metrics and stores them so you can query/graph/alert.
	\item Adapter (KEDA or Prometheus Adapter): the translator for Kubernetes. It takes Prometheus metrics and exposes them to Kubernetes' autoscaling APIs so HPAs can use them.
		\begin{itemize}
			\item By default, Kubernetes only knows basic resource metrics (CPU
				\& memory) via metrics.k8s.io.
			\item To scale on custom metrics (like GPU utilization), you must
				expose them via: custom.metrics.k8s.io (object-scoped) or
				external.metrics.k8s.io (external/global), through one adapter
				per API (limitation of the design).
		\end{itemize}
	\item HPA (Horizontal Pod Autoscaler): the replica dial. It looks at a metric (e.g., GPU utilization) and scales Pods up/down.
	\item Cluster-level scaling (CA/NAP/Karpenter): the machine buyer. If more Pods need room, these can add nodes (and later remove them). NAP can even create new node *types* if needed.
\end{itemize}

The flow with GPUs:
\begin{itemize}
	\item DCGM exports GPU metrics.
	\item Prometheus scrapes and stores them.
	\item KEDA (or Prometheus Adapter) publishes those metrics to Kubernetes as autoscaling metrics.
	\item HPA reads that metric and scales Pod replicas.
	\item If Pods don't fit on current nodes, Cluster Autoscaler / NAP may add nodes (and later remove them).
\end{itemize}


\subsection{Rolling Updates}

Rolling updates or rolling upgrades is a strategy that gradually implements the new version of an application to reduce downtime and maximize agility. It works by gradually creating new instances and turning off the old ones, replacing them in a methodical manner. This update approach allows the system to remain functional and accessible to users even during the update process, otherwise known as zero downtime. Rolling updates also make it easier to catch bugs before they have too much effect and rollback faulty deployments. 

Rolling updates is a feature built into k8s and another major reason for its wide-spread use and popularity. Kubernetes provides an automated and simplified way to carry out rolling updates. 

\subsection{Inference Graphs}

Inference graphs are the crème filling of a donut, the muffin top of a muffin, and the toppings on a pizza: they are just phenomenal. Inference graphs allow us to create sophisticated flow diagrams at inference in a resource-saving way. Consider figure 6.8, which shows us the building blocks for any inference graph. 

Generally, any time you have more than one model, it's useful to consider an infer- ence graph architecture. Your standard LLM setup is usually already at least two models: an encoder and the language model itself.

 Usually, when we see LLMs deployed in the wild, these two models are deployed together. You send text data to your system, and it returns generated text. It's often no big deal, but when deployed as a sequential inference graph instead of a packaged ser- vice, we get some added bonuses. First, the encoder is usually much faster than the LLM, so we can split them up since you may only need one encoder instance for every two to three LLM instances. Encoders are so small that this doesn't necessarily help us out that much, but it saves the hassle of redeploying the entire LLM if we decide to deploy a new encoder model version. In addition, an inference graph will set up an individual API for each model, which allows us to hit the LLM and encoder separately.

This is really useful if we have a bunch of data we'd like to preprocess and save in a VectorDB; we can use the same encoder we already have deployed. We can then pull this data and send it directly into the LLM.

The biggest benefit of an inference graph is that it allows us to separate the API and the LLM. The API sitting in front of the LLM is likely to change much more often as you tweak prompts, add features, and fix bugs. The ability to update the API with out having to deploy the LLM will save your team a lot of effort.

Let's now consider figure 6.9, which provides an example inference graph deploy- ment using Seldon. In this example, we have an encoder model, an LLM, a classifier model, and a simple API that combines the results. Whereas we would have to build a container and the interface for each of these models, Seldon creates an orchestrator that handles communication between a user's request and each node in the graph.

\paragraph{Summary} Don't ship your entire AI pipeline as one big black-box service. Instead, split it into small model services and wire them together as a graph at inference time.

\subsection{Monitoring}

\section{Production Challenges}

\subsection{Model updates and retraining}
\subsection{Load testing}
\subsection{Troubleshooting poor latency}

One of the biggest bottlenecks when it comes to your model's performance in terms of latency and throughput has nothing to do with the model itself but comes from \textbf{data transmission of the network}. One of the simplest methods to improve this I/O constraint is to \textbf{serialize the data before sending it across the wire}, which can have a large effect on ML workloads where the payloads tend to be larger, including LLMs where prompts tend to be long.

To serialize the data, we utilize a framework known as \textit{Google Remote Procedure Call (gRPC)}. gRPC is an API protocol similar to REST, but instead of sending JSON objects, we \textbf{compress the payloads into a binary serialized format using Protocol Buffers}, also known as \textit{protobufs}. By doing this, we can send more information in fewer bytes, which can easily give us orders of magnitude improvements in latency. Luckily, most inference services will implement gRPC along with their REST counterparts right out of the box, which is extremely convenient since the major hurdle to using gRPC is setting it up. 

A major reason for this convenience is the Seldon V2 Inference Protocol, which is widely implemented. The only hurdle, then, is ensuring our client can serialize and deserialize messages to take advantage of the protocol. In listing 6.13, we show an example client using MLServer to do this. It's a little bit more in depth than your typi- cal curl request, but a closer inspection shows the majority of the complexity is simply converting the data from different types as we serialize and deserialize it.

\subsection{Resource management}
\subsection{Cost engineering}

