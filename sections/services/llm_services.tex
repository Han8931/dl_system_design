\chapter{LLM Services: A Practical Guide}

\textit{Production} refers to the phase where the model is integrated into live or operational environment to perform its intended tasks or provide services to end users. It's a crucial phase in making the model available for real-world applications and services. In this chapter, we will explore how to package up an LLM into a service or API so that it can take on-demand requests. Then, we will study how to set up a cluster in the cloud where you can deploy this service. 

\section{Creating an LLM service}

\subsection{Model Compilation}
The success of any model in production is dependent on the hardware it runs on. Unfortunately, when programming in a high-level language like Python-based frameworks like PyTorch or TensorFlow, the model won't be optimized to take full advantage of the hardware. This is where compiling comes into play. Compiling is the process of taking code written in a high-level language and converting or lowering it to machine-level code that the computer can process quickly. Compiling your LLm can easily lead to major inference and cost improvements. 

\paragraph{Kernel Tuning} In DL, and high-performance computing, a kernel is a small program or function designed to run on a GPU or other similar processors. These routines are developed by the hardware vendor to maximize chip efficiency. 

During kernel tuning, the most suitable kernels are chosen from a large collection of highly optimized kernels. 


\paragraph{Kernel Fusion}
A \textbf{GPU kernel} is the tiny function that runs on each element. \textbf{Fusion} $=$ do several elementwise ops in one kernel so each element is read from VRAM once, processed in registers, then written back once.

\begin{commentbox}{The \texttt{y = ReLU(x + b)} example}
Assume \texttt{x} and \texttt{b} are the same length (or \texttt{b} is broadcast on the last dim).

Without fusion (two kernels: add, then ReLU):
\begin{enumerate}
	\item For every element i:
	\item Read x[i] from VRAM
	\item Read b[i] from VRAM
	\item Compute t = x[i] + b[i]
	\item Write t to VRAM ← intermediate array
	\item Read t from VRAM
	\item Compute y = max(t, 0)
	\item Write y to VRAM
\end{enumerate}
Global-memory ops per element: 5 (read x, read b, write t, read t, write y)
Two kernel launches (one for add, one for ReLU).

With fusion (one kernel: add+ReLU together)
\begin{enumerate}
	\item For every element i:
	\item Read x[i] from VRAM
	\item Read b[i] from VRAM
	\item Compute t = x[i] + b[i] (kept in a register, not VRAM)
	\item Compute y = max(t, 0)
	\item Write y to VRAM
\end{enumerate}
Global-memory ops per element: 3 (read x, read b, write y)
One kernel launch.
\end{commentbox}

ReLU Example:
\begin{lstlisting}[language=Python]
import torch, time

B, C = 4096, 4096
x = torch.randn(B, C, device="cuda", dtype=torch.float32)
b = torch.randn(C,     device="cuda", dtype=torch.float32)

def add_relu_unfused(x, b):
    # Eager mode typically launches two kernels (add, then relu)
    return torch.relu(x + b)

# PyTorch 2.x compiler (Inductor) generates fused kernels automatically
# This fuses common elementwise chains (like add→ReLU) into single kernels.
add_relu_fused = torch.compile(add_relu_unfused, backend="inductor")

# Correctness check
with torch.no_grad():
    y1 = add_relu_unfused(x, b)
    y2 = add_relu_fused(x, b)
    print("max |diff| =", (y1 - y2).abs().max().item())

# Simple timing
def bench(fn, iters=50, warmup=10):
    for _ in range(warmup):
        fn(x, b); torch.cuda.synchronize()
    t0 = time.perf_counter()
    for _ in range(iters):
        fn(x, b)
    torch.cuda.synchronize()
    return (time.perf_counter() - t0) * 1000 / iters

print("Unfused  :", bench(add_relu_unfused), "ms/iter")
print("Fused    :", bench(add_relu_fused),  "ms/iter")
\end{lstlisting}


\paragraph{Graph Optimization}
Graph optimization = semantics-preserving rewrites of the op graph (fold, fuse, simplify, relayout, retype, reschedule) so the lowered kernels do less work with less memory traffic

\paragraph{TensorRT} NVIDIA TensorRT is an SDK that converts a trained model into an optimized ``engine'' and then runs that engine with very low latency/high throughput on NVIDIA GPUs. It includes an optimizer (compiler) and a lightweight runtime.

How it works (typical workflow):
\begin{enumerate}
	\item Import your model (usually ONNX; there are parsers and framework bridges).
	\item Build an optimized engine: TensorRT selects fast kernels ("tactics"), fuses layers, lowers precision (FP16/INT8) if allowed, and specializes to your shapes/hardware.
	\item Serialize the engine to a .plan file (so you can load it instantly in production).
	\item Run it via the TensorRT runtime (C++/Python).
\end{enumerate}

\begin{lstlisting}[language=Python]
import tensorrt as trt

logger  = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(logger)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser  = trt.OnnxParser(network, logger)

with open("model.onnx", "rb") as f:
    assert parser.parse(f.read()), parser.get_error(0)

config = builder.create_builder_config()
config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 << 30)  # workspace budget
config.set_flag(trt.BuilderFlag.FP16)  # enable mixed precision if supported

# Dynamic-shape profile for input "input" (name must match your ONNX)
profile = builder.create_optimization_profile()
profile.set_shape("input", min=(1,3,224,224), opt=(8,3,224,224), max=(32,3,224,224))
config.add_optimization_profile(profile)

engine = builder.build_engine(network, config)
with open("model.plan", "wb") as f:
    f.write(engine.serialize())
\end{lstlisting}



\paragraph{ONNX Runtime}

\textbf{ONNX}, which stands for Open Neural Network Exchange, is an open source format and ecosystem designed for representing and interoperating between different deep learning frameworks. It was created to address the challenge of model portability and compatibility. ONNX is an IR (intermediate representation) and it allows you to represent models trained in some deep learning framework (\eg TensorFlow, PyTorch) in a standardized format easily consumed by other frameworks and it facilitates the exchange of models between different tools and environments. Unlike TensorRT, ONNX Runtime is intended to be hardware-agnostic, meaning it can be used with a variety of hardware accelerators, including CPUs, GPUs, and specialized hardware like TPUs. 

\subsection{LLM storage strategies}

Now we have a compiled model, we need to think about how our service will access it.This step is critical, because boot times can be a nightmare when working with LLMs since it can take a long time to load such large assets into memory.  

Object storage systems break up assets into small franctional bits called objects. They allow us to federate the entire asset across multiple machines and physical memory locations, a powerful tool that powers the clound, and to cheaply store large objects on hardware.  

\paragraph{Fusing} is the process of mounting a bucket to your machine as if it were an external hard drive. 

\paragraph{Baking the Model}

Baking is the process of putting your model into the Docker image. It is considered an \textit{anti-pattern}. 
\begin{itemize}
	\item Gigantic images: every update means pushing/pulling multi-GB layers → slow CI/CD, slow Kubernetes rollouts, higher storage costs.
	\item Tight coupling: a new model version requires a full image rebuild and redeploy (harder A/B tests, slower rollbacks, no "one image, many models").
	\item Poor caching: one byte change in weights invalidates a huge layer; your build cache won't help much.
	\item Security issue
\end{itemize}

\paragraph{Hybrid: download once, reuse many}

Download the model at boot time but store it in a volume that is mounted at boot time. While this doesn't help at all with the first deployment in a region, it does substantially help any new instances, as they can simply mount this same volume and have the model available to load without having to download. 

\begin{itemize}
	\item At boot (when the service starts up), your service (or an init step) pulls the model from S3/MinIO/HF/etc.
	\item It stores the files on a persistent volume that's mounted into the container.
	\item Subsequent pods/containers on the same node (or across nodes if using a shared RWX volume) just mount the volume—no re-download.
	\item You keep your app image small and decouple model updates from image builds.
\end{itemize}


\subsection{Adaptive request batching}
A typical API will accept and process requests in the order they are received, processing them immediately and as quickly as possible. However, anyone who's trained a ML model has come to realize that there are mathematical and computational advantages to running inference in batches of powers of 2 (16, 32, 64, etc), particularly when GPUs are involved, where we can take advantage of better memory alignment or vectorized instructions parallelizing computations across the GPU cores.  

What adaptive batching does is essentially pool requests together over a certain period of time. Once the pool receives the configured maximum batch size or the timer runs out, it will run inference on the entire batch through the model, sending the results back to the individual clients that requested them. Essentially, it's a queue. Setting one up yourself can and will be a huge pain; thankfully, most ML inference services offer this out of the box, and almost all are easy to implement. For example, in BentoML, add \texttt{@bentoml.Runnable.method(batchable=True)} as a decorator to your predict function, and in Triton Inference Server, add \texttt{dynamic\_batching {}} at the end of your model definition file.

If that sounds easy, it is. Typically, you don't need to do any further finessing, as the defaults tend to be very practical. That said, if you are looking to maximize every bit of efficiency possible in the system, you can often set a maximum batch size, which will tell the batcher to run once this limit is reached, or a batch delay, which does the same thing but for the timer. Increasing either will result in longer latency but likely better throughput, so typically these are only adjusted when your system has plenty of latency budget.

Overall, the benefits of adaptive batching include better use of resources and higher throughput at the cost of a bit of latency. This is a valuable trade-off, and we recommend giving your product the latency bandwidth to include this feature. In our experience, optimizing for throughput leads to better reliability and scalability and thus greater customer satisfaction. Of course, when latency times are extremely important or traffic is few and far between, you may rightly forgo this feature.


\subsection{Flow Control}

\subsection{Streaming responses}
\subsection{Feature store}
\subsection{Retrieval-augmented generation}
\subsection{LLM service libraries}

\section{Setting up infrastructure}
\subsection{Provisioning clusters}
