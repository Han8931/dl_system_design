\chapter{Introduction}

\section{Operations challenges with LLMs}
\begin{itemize}
	\item \textbf{Long download times} (\eg Bloom LLM is 330GB).
	\item \textbf{Longer deploy times} (\eg Bloom takes $30\sim 45$ mins to load the model into GPU).
	\item Along with increases in model size often come increases in \textbf{inference latency}. 
	\item \textbf{Managing GPUs}
	\item \textbf{Peculiarities of text data}: unlike other fields, texts have ambiguities. 
	\item \textbf{Token limits for a model} create bottlenecks
	\item \textbf{Hallucinations cause confusion} 
	\item \textbf{Bias and ethical considerations}
	\item \textbf{Security concerns}
	\item \textbf{Controlling costs}: \eg GPUs, infra, storage, operational costs like energy consumption during both training and inference. 
\end{itemize}

\section{LLMOps Essentials}

\begin{itemize}
	\item \textbf{Compression} is the practice of making models smaller. 
	\item \textbf{Quantizing} is the process of reducing precision in preference of lowering the memory requirements. 
\end{itemize}




