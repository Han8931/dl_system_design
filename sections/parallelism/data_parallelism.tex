\chapter{Data Parallelism}

\begin{figure}[t]
	\centering
	\includegraphics[scale=1.5]{./images/data_parallel.pdf}
\end{figure}

\section{Data Parallel}
\label{sec:parallelism:data_parallelism:dp}

The first step of the typical training loop for deep learning models is to split a dataset into batches so that we can feed them into the model and compute gradients corresponding to them. As the model size grows up, we couldn't fit the model into a single GPU. The \textit{data parallelism} tries to tackle the issue by clone the model across multiple GPUs so that each GPU can take a small portion of the batches for each iteration. Data Parallel (sometimes referred to as ``single-node data parallel'') is typically used when you have \textbf{multiple GPUs on a single machine}. 

Let's say the batch size is 10 and we have 5 GPUs. Then, each GPU takes 2 batches and calculate gradients by on its own. The calculated gradients are then synchronized across the GPUs pretending they are computed on a single GPU. Finally, the synchronized gradient information is going to be distributed to them. 

There are some important things to mention: 
\begin{enumerate}
	\item One process (or master thread) becomes a bottleneck for gradient aggregation and parameter updates.
	\item As you increase the number of GPUs, or try to involve multiple machines, communication overhead grows significantly and can slow down training.
	\item Each GPU holds a copy of the entire model, which can be large.
\end{enumerate}

\section{Distributed Data Parallel}
\label{sec:parallelism:data_parallelism:ddp}
To alleviate such issues, we can adopt an approach called \textit{Distributed Data Parallel} (DDP), which is designed to scale training across many GPUs, potentially across multiple machines (nodes). Modern deep learning frameworks (like PyTorch torch.nn.parallel.DistributedDataParallel) typically recommend DDP as the best practice for multi-GPU/multi-node training due to better performance and scalability. During backpropagation, gradients are shared among GPUs through efficient communication primitives, resulting in synchronized model parameters across all GPUs.

Key benefits:
\begin{itemize}
	\item Scalability: You can increase the number of GPUs (and even add more machines) to handle large datasets and bigger models.
	\item Performance: DDP typically provides better performance than older methods like \textsc{nn.DataParallel} (in PyTorch) because it uses \textit{all-reduce} and eliminates the single ``master'' bottleneck.
	\item Flexibility: You can combine DDP with other parallelization strategies (\eg model parallel, sharded data parallel, pipeline parallel) if needed.
\end{itemize}

\subsection{Concepts and Terminology}

All-Reduce is a collective communication operation commonly used in distributed computing (especially in high-performance computing and deep learning). In simple terms:
\begin{itemize}
	\item Each process (or GPU) starts with its own data (\eg local gradients).
	\item These data are combined (usually via a reduction operation like sum, mean, min, or max) across all processes.
	\item The result of that reduction (\eg the summed gradients) is then shared back so that every process receives the same reduced value.
	\item Hence the name: ``all'' (everyone gets the result) + ``reduce'' (combine data).
\end{itemize}

Basic Terms:
\begin{itemize}
	\item World Size: The total number of processes engaged in the distributed job. Often, we run one process per GPU, so world size is the number of GPUs.
	\item Rank: A unique integer ID assigned to each process. Ranks typically range from 0 to $\text{world\_size} - 1$. Rank 0 is often referred to as the ``leader'' or ``master'' process, but in DDP, every process does roughly the same work.
	\item Local Rank: When multiple GPUs reside on a single node, local rank identifies which GPU a specific process is mapped to on that local machine (\eg 0 for the first GPU, 1 for the second, etc.).
	\item Backend: The communication backend used for synchronization (\eg nccl). For GPU training, NCCL is typically recommended because it's optimized for high-performance GPU-to-GPU communication.
	\item Initialization Method: Describes how processes connect with each other (\eg a TCP store, a file-based store). This allows all processes to know who's who in the cluster.
\end{itemize}

\subsection{How DDP Works Under the Hood}
\begin{enumerate}
	\item  Process Per GPU: Each GPU runs the same script in its own process.  
	\item  Data Subset: A DistributedSampler ensures that each process sees a unique subset of data. This prevents overlap in data usage among GPUs.  
	\item  Full Model Copy: Each GPU has a full replica of the model in memory.  
		\begin{itemize}
			\item For massive models, consider \textit{Sharded DDP} (\eg PyTorch's FSDP or DeepSpeed ZeRO) to split parameters across GPUs.
		\end{itemize}
	\item  All-Reduce Gradient Sync: After backprop, gradients are summed (or averaged) across processes with an all-reduce operation. This keeps all models in sync.
\end{enumerate}


\section{DDP vs DataParallel}

In PyTorch there are two common ways to use multiple GPUs:

\begin{itemize}
    \item \textbf{\texttt{nn.DataParallel}}: ``Single process, many GPUs, GPU0 is the boss.''
    \item \textbf{\texttt{DistributedDataParallel} (DDP)}: ``One process per GPU, talk via all-reduce (NCCL).''
\end{itemize}

In practice, \texttt{nn.DataParallel} is easy to use but inefficient and considered outdated for serious training. \texttt{DistributedDataParallel} is the recommended approach, especially for large models and LLMs.

\subsection{Mental Model}

\subsubsection{\texttt{nn.DataParallel}}

\begin{itemize}
    \item There is \textbf{one Python process} and one ``master'' model copy on GPU0.
    \item For each forward pass:
    \begin{enumerate}
        \item The input batch is split across GPUs.
        \item The model on GPU0 is \emph{replicated} onto the other GPUs.
        \item Forward and backward are run on each GPU.
        \item Gradients are gathered back to GPU0, summed, and \verb|optimizer.step()| is applied on GPU0's parameters.
    \end{enumerate}
    \item GPU0 becomes a bottleneck:
    \begin{itemize}
        \item It hosts the master model.
        \item It handles scatter/gather and the optimizer step.
    \end{itemize}
    \item Everything happens inside a single Python process, so the Python GIL can also limit scaling.
\end{itemize}

\subsubsection{\texttt{DistributedDataParallel} (DDP)}

\begin{itemize}
    \item There is \textbf{one process per GPU} (or per GPU per node).
    \item Each process:
    \begin{itemize}
        \item Owns a model replica on its GPU.
        \item Receives a shard of the global batch.
    \end{itemize}
    \item After \verb|loss.backward()|:
    \begin{itemize}
        \item Gradients are \textbf{averaged across processes} via an all-reduce operation (typically NCCL).
        \item Each process then calls \verb|optimizer.step()| locally using the synchronized gradients.
    \end{itemize}
    \item There is no single central bottleneck; synchronization is done in a distributed manner.
\end{itemize}

\subsection{Practical Differences}

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Aspect} & \textbf{\texttt{nn.DataParallel}} & \textbf{\texttt{DistributedDataParallel}} \\
\hline
Processes & 1 & 1 per GPU \\
\hline
Model copies & Replicated each forward & One static replica per process \\
\hline
Gradient sync & Gather on GPU0 & All-reduce (NCCL) \\
\hline
Python GIL bottleneck & Yes & No (multi-process) \\
\hline
Multi-node support & No & Yes \\
\hline
Performance / scaling & Poor for large models / many GPUs & Good, recommended \\
\hline
Recommended for training & No & Yes \\
\hline
\end{tabular}
\end{center}

For large-scale ML and LLM training/fine-tuning, \texttt{DistributedDataParallel} is the default choice.

