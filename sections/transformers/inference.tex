Most widely used decoder-only LLMs (\eg GPT) are trained with a causal language modeling objective—effectively, next-token prediction. Given an input token sequence, they generate the continuation autoregressively until a stopping criterion is reached (\eg a maximum length) or a special \textsc{<end>} token is produced. The inference process naturally splits into two stages: \Ni the \textit{prefill} phase, which reads users inputs and \Nii the \textit{decode} phase, which generates responses.

\paragraph{Tokens.}
Tokens are the smallest units the model consumes. A common rule of thumb is that one token corresponds to roughly four English characters (the exact ratio depends on the tokenizer). Text is always tokenized before entering the model.

\section{Prefill Phase}

During prefill, the model ingests the entire input sequence (\eg user/system prompt) and constructs the attention memory—namely the keys and values, often called the \emph{KV cache}—to be reused during generation. Because the full input is available from the start, this stage parallelizes well: it resembles a matrix–matrix workload and typically drives the GPU close to peak utilization.

\section{Decoding Phase}

Decoding then emits output one token at a time. Each new token must attend to all tokens seen so far (the original prompt plus previously generated outputs), which makes this step inherently sequential. At each step, the computation behaves more like matrix–vector multiplication and generally uses the GPU less efficiently than prefill.

For every token produced, the model must \emph{fetch} a substantial portion of the KV cache from GPU memory. As the context grows, the data transferred per step increases, so latency is governed more by memory bandwidth than by raw compute—decoding is thus \emph{memory-bound}.

\textbf{Intuition.} Imagine typing an answer character by character, but before each keystroke you skim all prior notes to remain consistent. The typing (compute) is quick; the skimming (memory access) is what slows you down.

For instance, with a $1{,}000$-token prompt, generating 200 tokens means token \#1 attends over roughly $1{,}000$ tokens, while token \#200 attends over roughly $1{,}200$ tokens.

Because decoding is the usual bottleneck, many inference optimizations focus here: more efficient attention mechanisms, improved KV-cache handling (\eg paging, quantization, head sharing), and strategies such as speculative or look-ahead decoding that reduce sequential work.

\section{Batching}

The simplest way to improve GPU utilization, and effectively throughput, is through batching. Since multiple requests use the same model, the memory cost of the weights is spread out. Larger batches getting transferred to the GPU to be processed all at once will leverage more of the compute available. 

Batch sizes, however, can only be increased up to a certain limit, at which point they may lead to a memory overflow. To better understand why this happens requires looking at key-value (KV) caching and LLM memory requirements.

Traditional batching (also called static batching) is suboptimal. This is because for each request in a batch, the LLM may generate a different number of completion tokens, and subsequently they have different execution times. As a result, all requests in the batch must wait until the longest request is finished, which can be exacerbated by a large variance in the generation lengths. There are methods to mitigate this, such as in-flight batching, which will be discussed later.

\section{KV-Caching}

One common optimization for the decode phase is \textit{KV caching}. The decode phase generates a single token at each time step, but each token depends on the key and value tensors of all previous tokens (including the input tokens' KV tensors computed at prefill, and any new KV tensors computed until the current time step). 

To avoid recomputing all these tensors for all tokens at each time step, it's possible to cache them in GPU memory. Every iteration, when new elements are computed, they are simply added to the running cache to be used in the next iteration. In some implementations, there is one KV cache for each layer of the model.

\section{LLM memory requirement }

In effect, the two main contributors to the GPU LLM memory requirement are model weights and the KV cache:
\begin{itemize}
	\item \textbf{Model weights}: Memory is occupied by the model parameters. As an example, a model with 7 billion parameters (such as Llama 2 7B), loaded in 16-bit precision (FP16 or BF16) would take roughly 7B $\times$ sizeof(FP16) $\approx 14$ GB in memory.
	\item \textbf{KV caching}: Memory is occupied by the caching of self-attention tensors to avoid redundant computation.
\end{itemize}

With batching, the KV cache of each of the requests in the batch must still be allocated separately, and can have a large memory footprint. The formula below delineates the size of the KV cache, applicable to most common LLM architectures today.
\begin{align*}
	&\text{Size of KV cache per token in bytes }=\\
	&2 \times (\text{n\_layers}) \times (\text{n\_heads} \times \text{dim\_head}) \times  (\text{precision\_in\_bytes})
\end{align*}
\begin{itemize}
	\item The first factor of 2 accounts for the $K$ and $V$ matrices. 
	\item Commonly, the value of (num\_heads $\times$ dim\_head) is equal the hidden\_size (or dimension of the model, d\_model) of the transformer. These model attributes are commonly found in model cards or associated config files.
\end{itemize}
This memory size is required for each token in the input sequence, across the batch of inputs. 

For example, with a Llama2 7B model in 16-bit precision and a batch size of 1, the size of the KV cache will be $1 \times 4096 \times 2 \times 32 \times 4096 \times 2$ bytes, where 4096 is the sequence length. In sum, it takes around 2 GB.

Managing this KV cache efficiently is a challenging endeavor. Growing linearly with batch size and sequence length, the memory requirement can quickly scale. Consequently, it limits the throughput that can be served, and poses challenges for long-context inputs. This is the motivation behind several optimizations featured.

\section{Scaling up LLMs with model parallelization}

One way to reduce the per-device memory footprint of the model weights is to distribute the model over several GPUs. Spreading the memory and compute footprint enables running larger models, or larger batches of inputs. Model parallelization is a necessity to train or infer on a model requiring more memory than available on a single device, and to make training times and inference measures (latency or throughput) suitable for certain use cases. There are several ways of parallelizing the model based on how the model weights are split. 

\begin{itemize}
	\item Pipeline parallelism
	\item Tensor parallelism
	\item Sequence parallelism
\end{itemize}

\subsection{Pipeline parallelism}

\textit{Pipeline parallelism} involves sharding the model (vertically) into chunks, where each chunk comprises \textbf{a subset of layers that is executed on a separate device}.

The main limitation of this method is that, due to the sequential nature of the processing, some devices or layers may remain idle while waiting for the output (activations, gradients) of previous layers. This results in inefficiencies or \textit{pipeline bubbles} in both the forward and backward passes.

\subsection{Tensor Parallelism}

Tensor parallelism involves sharding (horizontally) individual layers of the model into smaller, independent blocks of computation that can be executed on different devices. Attention blocks and multi-layer perceptron (MLP) layers are major components of transformers that can take advantage of tensor parallelism. In multi-head attention blocks, each head or group of heads can be assigned to a different device so they can be computed independently and in parallel.  

\subsection{Sequence Parallelism}

Tensor parallelism has limitations, as it requires layers to be divided into independent, manageable blocks. It's not applicable to operations like LayerNorm and Dropout, which are instead replicated across the tensor-parallel group. While LayerNorm and Dropout are computationally inexpensive, they do require a considerable amount of memory to store (redundant) activations.

Sequence parallelism (SP) splits work along the sequence length dimension across multiple GPUs. Instead of every GPU holding all tokens of each sequence, each GPU holds a slice of tokens (\eg tokens 0–255 on GPU0, 256–511 on GPU1). It's usually used together with tensor/model parallelism to cut activation memory and enable longer context or larger batches.



\section{Optimizing the attention mechanism}

\begin{itemize}
	\item Multi-head attention
	\item Multi-query attention
	\item Grouped-query attention
	\item Flash attention
\end{itemize}



\section{Model optimization techniques}

\subsection{Quantization}

Quantization is the process of reducing the precision of a model's weights and activations. Most models are trained with 32 or 16 bits of precision, where each parameter and activation element takes up 32 or 16 bits of memory—a single-precision floating point. However, most deep learning models can be effectively represented with eight or even fewer bits per value.  

Reducing the precision of a model can yield several benefits. If the model takes up less space in memory, you can fit larger models on the same amount of hardware. Quantization also means you can transfer more parameters over the same amount of bandwidth, which can help to accelerate models that are bandwidth-limited. 

There are many different quantization techniques for LLMs involving reduced precision on either the activations, the weights, or both. It's much more straightforward to quantize the weights because they are fixed after training. However, this can leave some performance on the table because the activations remain at higher precisions. GPUs don't have dedicated hardware for multiplying INT8 and FP16 numbers, so the weights must be converted back into a higher precision for the actual operations. 

It's also possible to quantize the activations, the inputs of transformer blocks and network layers, but this comes with its own challenges. Activation vectors often contain outliers, effectively increasing their dynamic range and making it more challenging to represent these values at a lower precision than with the weights. 

One option is to find out where those outliers are likely to show up by passing a representative dataset through the model, and choosing to represent certain activations at a higher precision than others (LLM.int8()). Another option is to borrow the dynamic range of the weights, which are easy to quantize, and reuse that range in the activations.

\subsection{Sparsity}

Similar to quantization, it's been shown that many deep learning models are robust to pruning, or replacing certain values that are close to 0 with 0 itself. Sparse matrices are matrices where many of the elements are 0. These can be expressed in a condensed form that takes up less space than a full, dense matrix.

GPUs in particular have hardware acceleration for a certain kind of structured sparsity, where two out of every four values are represented by zeros. Sparse representations can also be combined with quantization to achieve even greater speedups in execution. Finding the best way to represent large language models in a sparse format is still an active area of research, and offers a promising direction for future improvements to inference speeds.

\subsection{Distillation}


\section{Model Serving Techniques}

\subsection{In-Flight Batching}
\subsection{Speculative inference}
