Most widely used decoder-only LLMs (\eg GPT) are trained with a causal language modeling objective—effectively, next-token prediction. Given an input token sequence, they generate the continuation autoregressively until a stopping criterion is reached (\eg a maximum length) or a special \textsc{<end>} token is produced. The inference process naturally splits into two stages: \Ni the \textit{prefill} phase, which reads users inputs and \Nii the \textit{decode} phase, which generates responses.

\paragraph{Tokens.}
Tokens are the smallest units the model consumes. A common rule of thumb is that one token corresponds to roughly four English characters (the exact ratio depends on the tokenizer). Text is always tokenized before entering the model.

\section{Prefill Phase}

During prefill, the model ingests the entire input sequence (\eg user/system prompt) and constructs the attention memory—namely the keys and values, often called the \emph{KV cache}—to be reused during generation. Because the full input is available from the start, this stage parallelizes well: it resembles a matrix–matrix workload and typically drives the GPU close to peak utilization.

\section{Decoding Phase}

Decoding then emits output one token at a time. Each new token must attend to all tokens seen so far (the original prompt plus previously generated outputs), which makes this step inherently sequential. At each step, the computation behaves more like matrix–vector multiplication and generally uses the GPU less efficiently than prefill.

For every token produced, the model must \emph{fetch} a substantial portion of the KV cache from GPU memory. As the context grows, the data transferred per step increases, so latency is governed more by memory bandwidth than by raw compute—decoding is thus \emph{memory-bound}.

\textbf{Intuition.} Imagine typing an answer character by character, but before each keystroke you skim all prior notes to remain consistent. The typing (compute) is quick; the skimming (memory access) is what slows you down.

For instance, with a $1{,}000$-token prompt, generating 200 tokens means token \#1 attends over roughly $1{,}000$ tokens, while token \#200 attends over roughly $1{,}200$ tokens.

Because decoding is the usual bottleneck, many inference optimizations focus here: more efficient attention mechanisms, improved KV-cache handling (\eg paging, quantization, head sharing), and strategies such as speculative or look-ahead decoding that reduce sequential work.

\section{Batching}

The simplest way to improve GPU utilization, and effectively throughput, is through batching. Since multiple requests use the same model, the memory cost of the weights is spread out. Larger batches getting transferred to the GPU to be processed all at once will leverage more of the compute available. 

Batch sizes, however, can only be increased up to a certain limit, at which point they may lead to a memory overflow. To better understand why this happens requires looking at key-value (KV) caching and LLM memory requirements.

Traditional batching (also called static batching) is suboptimal. This is because for each request in a batch, the LLM may generate a different number of completion tokens, and subsequently they have different execution times. As a result, all requests in the batch must wait until the longest request is finished, which can be exacerbated by a large variance in the generation lengths. There are methods to mitigate this, such as in-flight batching, which will be discussed later.

\section{KV-Caching}

One common optimization for the decode phase is \textit{KV caching}. The decode phase generates a single token at each time step, but each token depends on the key and value tensors of all previous tokens (including the input tokens' KV tensors computed at prefill, and any new KV tensors computed until the current time step). 

To avoid recomputing all these tensors for all tokens at each time step, it's possible to cache them in GPU memory. Every iteration, when new elements are computed, they are simply added to the running cache to be used in the next iteration. In some implementations, there is one KV cache for each layer of the model.

\section{LLM memory requirement }

In effect, the two main contributors to the GPU LLM memory requirement are model weights and the KV cache:
\begin{itemize}
	\item \textbf{Model weights}: Memory is occupied by the model parameters. As an example, a model with 7 billion parameters (such as Llama 2 7B), loaded in 16-bit precision (FP16 or BF16) would take roughly 7B $\times$ sizeof(FP16) $\approx 14$ GB in memory.
	\item \textbf{KV caching}: Memory is occupied by the caching of self-attention tensors to avoid redundant computation.
\end{itemize}

With batching, the KV cache of each of the requests in the batch must still be allocated separately, and can have a large memory footprint. The formula below delineates the size of the KV cache, applicable to most common LLM architectures today.
\begin{align*}
	&\text{Size of KV cache per token in bytes }=\\
	&2 \times (\text{n\_layers}) \times (\text{n\_heads} \times \text{dim\_head}) \times  (\text{precision\_in\_bytes})
\end{align*}
\begin{itemize}
	\item The first factor of 2 accounts for the $K$ and $V$ matrices. 
	\item Commonly, the value of (num\_heads $\times$ dim\_head) is equal the hidden\_size (or dimension of the model, d\_model) of the transformer. These model attributes are commonly found in model cards or associated config files.
\end{itemize}
This memory size is required for each token in the input sequence, across the batch of inputs. 

For example, with a Llama2 7B model in 16-bit precision and a batch size of 1, the size of the KV cache will be $1 \times 4096 \times 2 \times 32 \times 4096 \times 2$ bytes, where 4096 is the sequence length. In sum, it takes around 2 GB.

Managing this KV cache efficiently is a challenging endeavor. Growing linearly with batch size and sequence length, the memory requirement can quickly scale. Consequently, it limits the throughput that can be served, and poses challenges for long-context inputs. This is the motivation behind several optimizations featured.

\section{Scaling up LLMs with model parallelization}

One way to reduce the per-device memory footprint of the model weights is to distribute the model over several GPUs. Spreading the memory and compute footprint enables running larger models, or larger batches of inputs. Model parallelization is a necessity to train or infer on a model requiring more memory than available on a single device, and to make training times and inference measures (latency or throughput) suitable for certain use cases. There are several ways of parallelizing the model based on how the model weights are split. 

\begin{itemize}
	\item Pipeline parallelism
	\item Tensor parallelism
	\item Sequence parallelism
\end{itemize}

\subsection{Pipeline parallelism}

\textit{Pipeline parallelism} involves sharding the model (vertically) into chunks, where each chunk comprises \textbf{a subset of layers that is executed on a separate device}.

The main limitation of this method is that, due to the sequential nature of the processing, some devices or layers may remain idle while waiting for the output (activations, gradients) of previous layers. This results in inefficiencies or \textit{pipeline bubbles} in both the forward and backward passes.

\subsection{Tensor Parallelism}

Tensor parallelism involves sharding (horizontally) individual layers of the model into smaller, independent blocks of computation that can be executed on different devices. Attention blocks and multi-layer perceptron (MLP) layers are major components of transformers that can take advantage of tensor parallelism. In multi-head attention blocks, each head or group of heads can be assigned to a different device so they can be computed independently and in parallel.  

\subsection{Sequence Parallelism}

Tensor parallelism has limitations, as it requires layers to be divided into independent, manageable blocks. It's not applicable to operations like LayerNorm and Dropout, which are instead replicated across the tensor-parallel group. While LayerNorm and Dropout are computationally inexpensive, they do require a considerable amount of memory to store (redundant) activations.

Sequence parallelism (SP) splits work along the sequence length dimension across multiple GPUs. Instead of every GPU holding all tokens of each sequence, each GPU holds a slice of tokens (\eg tokens 0–255 on GPU0, 256–511 on GPU1). It's usually used together with tensor/model parallelism to cut activation memory and enable longer context or larger batches.



\section{Optimizing the attention mechanism}

\begin{itemize}
	\item Multi-head attention
	\item Multi-query attention
	\item Grouped-query attention
	\item Flash attention
\end{itemize}


\subsection{Flash Attention}

\begin{itemize}
	\item    Fast — excerpt from the paper: "We train BERT-large (seq. length 512) 15% faster than the training speed record in MLPerf 1.1, GPT2 (seq. length 1K) 3x faster than baseline implementations from HuggingFace and Megatron-LM, and long-range arena (seq. length 1K-4K) 2.4x faster than baselines."
	\item    Memory-efficient — compared to vanilla attention, which is quadratic in sequence length, O(N²), this method is sub-quadratic/linear in N (O(N)). We'll see later why & how.
	\item    Exact — meaning it's not an approximation of the attention mechanism (like e.g. sparse, or low-rank matrix approximation methods) — its outputs are the same as in the "vanilla" attention mechanism.
	\item    IO aware — compared to vanilla attention, flash attention is sentient.
\end{itemize}

Over the years GPUs have been adding compute capacity (FLOPS) at a faster pace than increasing the memory throughput (TB/s).

It doesn't matter if you can compute at exaFLOPS speeds if there is no data to be processed. These 2 need to be closely aligned, and since the hardware lost that balance we have to make our software compensate for it.




We argue that a missing principle is making attention algorithms IO-aware – accounting for reads and writes between levels of GPU memory.


That is, modern GPUs have several types of memory:
\begin{itemize}
	\item SRAM – fast, on-chip, small
	\item HBM – slower than SRAM, large size. That's what we usually address as GPU memory.
\end{itemize}

 To conduct computation, data must be transferred from HBM to SRAM, and this transfer is not overhead-free!

The Flash Attention algorithm proposes a method of computing attention in tiles, without explicitly materializing the attention scores tensor. Here, materializing a matrix means that at any given time, the matrix exist in its full shape in memory.

It's easy to see that this matrix requires $O(n^2)$ of memory to store. For large sequence lengths, that's a lot of data. So, if we manage to avoid explicitly materializing this matrix, we can save lots of memory.

However, this matrix is necessary for transformer training as it is a part of backpropagation and gradient calculation. The authors propose that it's better to recalculate this matrix during the backward pass (again without explicit materialization). Not only does this saves lots of memory, but it also provides huge speedups as we don't need to transfer this enormous matrix between different GPU memory types.

Overall, such an approach did not only speed up calculations by taking GPU I/O specifics into account, but also allowed processing huge sequence lengths as memory complexity drops to $O(n)$.


In sum,
\begin{itemize}
	\item Load a small block of Q and K into SRAM.
	\item Compute just that block of scores (Q·Kᵀ).
	\item Do a streaming softmax: keep a running max and sum so softmax stays numerically stable without needing all tokens at once.
	\item Immediately apply that softmax block to the matching V block and accumulate partial outputs.
	\item Move to the next tile. When all tiles are processed, you already have the final output—no big attention matrix ever stored.
\end{itemize}
This is called an IO-aware algorithm: it minimizes slow memory traffic and maximizes use of the GPU's fast memory.

What attention normally does (and why it's slow)

Given per-head matrices:
\begin{itemize}
	\item$Q \in \mathbb{R}^{N\times d}, K \in \mathbb{R}^{N\times d}, V \in \mathbb{R}^{N\times d_v}$ 
	\item Scores: $S = \frac{QK^\top}{\sqrt{d}}  (\text{size } N\times N)$
	\item Output: $O = \mathrm{softmax}(S)V$
\end{itemize}
Naive kernels materialize $S$ (size ($N^2$)), apply softmax row-wise, then multiply by $V$.
Problem: writing/reading ($N^2$) scores to HBM (GPU DRAM) is \textit{memory-bound} and explodes memory as $N$ grows (\eg $4k$ tokens $\to$ $16M (4k^2)$ scores per head).

The core idea of FlashAttention is to compute over tile with stream and never form $S$ in HBM. In other words, do attention in tiles that fit in on-chip SRAM to minimize HBC traffic, and keep only tiny per-row summaries in HBM. For each tile:

\begin{enumerate}
	\item Load a block of $Q$ and a block of $K$, $V$ into SRAM.
	\item Compute partial scores $S_{\text{blk}} = Q_{\text{blk}}K_{\text{blk}}^\top/\sqrt{d}$.
	\item Apply a streaming softmax update so you don't need the whole row at once.
	\item Immediately multiply by $V_{\text{blk}}$ and accumulate partial
		outputs for that row block.
	\item Move to the next block of $K/V$ and repeat.
\end{enumerate}

\paragraph{The streaming (online) softmax trick}

For one output row $i$ (one query token), the softmax over all keys $j=1\ldots N$ is:
\begin{align*}
	o_i = \sum_{j=1}^N \frac{e^{s_{ij}}}{\sum_{k=1}^N e^{s_{ik}}} v_j,
	\quad s_{ij} = \frac{q_i\cdot k_j}{\sqrt{d}} + \text{mask/bias}.
\end{align*}
We process keys in chunks (\ie tiles). Maintain per-row running stats:

\begin{itemize}
	\item $m_i$: running max score seen so far (for numerical stability)
	\item $\ell_i$: running sum of exp-shifted scores, \ie $\ell_i = \sum e^{s_{ij}-m_i}$
	\item $z_i$: running weighted sum of values, $z_i = \sum e^{s_{ij}-m_i} v_j$
\end{itemize}
When you see a new block with per-row block max $m_i^{\text{blk}}=\max_j s_{ij}^{\text{blk}}$ and sums
\begin{itemize}
	\item $\ell_i^{\text{blk}} = \sum_j e^{s_{ij}^{\text{blk}}-m_i^{\text{blk}}},$
	\item $z_i^{\text{blk}} = \sum_j e^{s_{ij}^{\text{blk}}-m_i^{\text{blk}}} v_j,$
\end{itemize}
update with running stats:
\begin{align*}
	m_i^{\text{new}} &= \max(m_i, m_i^{\text{blk}}),\\
	\ell_i^{\text{new}} &= \ell_i e^{m_i - m_i^{\text{new}}}
	+
	\ell_i^{\text{blk}} e^{m_i^{\text{blk}} - m_i^{\text{new}}},\\
	z_i^{\text{new}} &= z_i e^{m_i - m_i^{\text{new}}}
	+
	z_i^{\text{blk}} e^{m_i^{\text{blk}} - m_i^{\text{new}}}.
\end{align*}
At the end of all blocks:
\begin{align*}
	o_i = \frac{z_i}{\ell_i}.
\end{align*}
classic log-sum-exp fusion with dynamic max shifting keeps numbers well-scaled without needing all $s_{ij}$ at once.


\begin{commentbox}{Example}
\begin{itemize}
	\item scores (two tiles): $s=[1,3|0,2]$
	\item 2-D values:
		\begin{itemize}
		  \item (v_1=[1,0])
		  \item (v_2=[0,2])
		  \item (v_3=[-1,1])
		  \item (v_4=[3,1])
		\end{itemize}
\end{itemize}
Define $a=e^{-2}\approx0.135335, b=e^{-1}\approx0.367879$.

\begin{itemize}
	\item \textbf{Tile 1}: $[1,3]$ with$[v_1,v_2]$ 
		\begin{itemize}
			\item block max: $m^{\text{blk}}=3$ 
			\item block sums (shift by $m^{\text{blk}}$):
				\begin{align*}
					\ell^{\text{blk}} &= e^{1-3}+e^{3-3} = a+1\\
					z^{\text{blk}} &= av_1 + 1\cdot v_2
					  = a[1,0]+[0,2] = [a,,2]
				\end{align*}
			\item merge into running stats (start from $m=-\infty,\ell=0,z=0$):
				\begin{align*}
					  m\leftarrow 3,\quad \ell\leftarrow a+1,\quad z\leftarrow [a,2].
				\end{align*}
		\end{itemize}
	\item \textbf{Tile 2}: $[0,2]$ with$[v_3,v_4]$ 
		\begin{itemize}
			\item block max: $m^{\text{blk}}=2$ 
			\item block sums:
				\begin{align*}
				  \ell^{\text{blk}}=a+1,\quad
				  z^{\text{blk}}=av_3+1\cdot v_4
				  = a[-1,1]+[3,1] = [3-a,1+a].
				\end{align*}
			\item merge (global max stays $m^{\text{new}}=\max(3,2)=3$):
				\begin{align*}
					\ell &\leftarrow \ell\cdot e^{3-3} + \ell^{\text{blk}}\cdot
					e^{2-3} = (a+1) + (a+1),b \approx \boxed{1.55300179}\\
				  z &\leftarrow z\cdot e^{3-3} + z^{\text{blk}}\cdot e^{2-3}
				  = [a,2] + b[3-a,1+a] \approx \boxed{[1.18918654,2.41766651]}.
				\end{align*}
		\end{itemize}
	\item \textbf{Finalize} (elementwise divide by scalar $\ell$)
		\begin{align*}
			o=\frac{z}{\ell}
			\approx \Big[\frac{1.18918654}{1.55300179},\frac{2.41766651}{1.55300179}\Big]
			= \boxed{[0.76573417,1.55676994]}.
		\end{align*}
	\item \textbf{Sanity check} (plain softmax on all 4 keys)
		Weights from $s=[1,3,0,2]$ are $\approx[0.08714,,0.64391,,0.03206,,0.23688]$.
		\begin{align*}
			\sum_j w_j v_j
			&= 0.08714[1,0]+0.64391[0,2]+0.03206[-1,1]+0.23688[3,1]\\
			&= [0.76573417,1.55676994],
		\end{align*}
		which matches the streaming result exactly. 
\end{itemize}
\end{commentbox}

% \begin{lstlisting}[language=Python]
% # Q: [nq, d], K: [nk, d], V: [nk, dv]
% # Tile sizes chosen so a Q tile and a K/V tile fit in SRAM.
% m = -inf * ones(nq)        # running max per row
% l = zeros(nq)              # running sum-exp per row
% z = zeros(nq, dv)          # running weighted sum per row

% for each KV_tile in partition_along_keys(K, V):
%     Kb, Vb = KV_tile  # [tk, d], [tk, dv]
%     for each Q_tile in partition_along_queries(Q):
%         Qb = Q_tile   # [tq, d]

%         # scores for this subproblem (live in SRAM)
%         Sb = (Qb @ Kb.T) / sqrt(d)           # [tq, tk]
%         Sb += local_mask_or_bias(Q_block, K_block)

%         # per-row block max and sums
%         mb = max_over_keys(Sb)               # [tq]
%         Pb = exp(Sb - mb[:, None])           # [tq, tk]   (not stored in HBM)
%         lb = sum_over_keys(Pb)               # [tq]
%         zb = Pb @ Vb                         # [tq, dv]

%         # merge into running stats (broadcast-safe)
%         m_new = maximum(m_block, mb)
%         l = l * exp(m_block - m_new) + lb * exp(mb - m_new)
%         z = z * exp(m_block[:, None] - m_new[:, None]) \
%             + zb * exp(mb[:, None] - m_new[:, None])
%         m = m_new

% # final normalize
% O = z / l[:, None]
% \end{lstlisting}

\section{Model optimization techniques}

\subsection{Quantization}

Quantization is the process of reducing the precision of a model's weights and activations. Most models are trained with 32 or 16 bits of precision, where each parameter and activation element takes up 32 or 16 bits of memory—a single-precision floating point. However, most deep learning models can be effectively represented with eight or even fewer bits per value.  

Reducing the precision of a model can yield several benefits. If the model takes up less space in memory, you can fit larger models on the same amount of hardware. Quantization also means you can transfer more parameters over the same amount of bandwidth, which can help to accelerate models that are bandwidth-limited. 

There are many different quantization techniques for LLMs involving reduced precision on either the activations, the weights, or both. It's much more straightforward to quantize the weights because they are fixed after training. However, this can leave some performance on the table because the activations remain at higher precisions. GPUs don't have dedicated hardware for multiplying INT8 and FP16 numbers, so the weights must be converted back into a higher precision for the actual operations. 

It's also possible to quantize the activations, the inputs of transformer blocks and network layers, but this comes with its own challenges. Activation vectors often contain outliers, effectively increasing their dynamic range and making it more challenging to represent these values at a lower precision than with the weights. 

One option is to find out where those outliers are likely to show up by passing a representative dataset through the model, and choosing to represent certain activations at a higher precision than others (LLM.int8()). Another option is to borrow the dynamic range of the weights, which are easy to quantize, and reuse that range in the activations.

\subsection{Sparsity}

Similar to quantization, it's been shown that many deep learning models are robust to pruning, or replacing certain values that are close to 0 with 0 itself. Sparse matrices are matrices where many of the elements are 0. These can be expressed in a condensed form that takes up less space than a full, dense matrix.

GPUs in particular have hardware acceleration for a certain kind of structured sparsity, where two out of every four values are represented by zeros. Sparse representations can also be combined with quantization to achieve even greater speedups in execution. Finding the best way to represent large language models in a sparse format is still an active area of research, and offers a promising direction for future improvements to inference speeds.

\subsection{Distillation}


\section{Model Serving Techniques}

\subsection{In-Flight Batching}
\subsection{Speculative inference}
